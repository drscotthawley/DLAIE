{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_DeeperNLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeJzNsGP41j"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1blkWFKMYqinrtTSKWO_Uvv7v_TvtX6zs?usp=sharing)\n",
        "\n",
        "# Lesson 8: Going Deeper into NLP\n",
        "\n",
        "---\n",
        "\n",
        "# Part I\n",
        "\n",
        "[Previously](https://github.com/drscotthawley/DLAIE/blob/main/Lessons/7_NLP_via_HuggingFace_Transformers.ipynb) we saw how convenient it was to use the `pipeline` method of the HuggingFace.co `transformers` library to perform a variety of Natural Language Processing (NLP) tasks. But there's a lot going on under the hood that was hidden from us.  If we want to learn how these models work, we're going to have to peel back several layers, on multiple levels.  \n",
        "\n",
        "What we did in the previous NLP lesson was a bit like watching a big rocket take off from a distance. There are many systems in the rocket that are all working together to effect the launch.  To understand how the big rocket operates, it will help if we go back to study smaller, simpler rockets so that we understand the principles of rocketry. \n",
        "\n",
        "In this lesson we'll learn the parts of an NLP model and see how they go together. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGWoRTrURumS"
      },
      "source": [
        "## 1. Tokenization\n",
        "\n",
        "Whatever NLP task we're interested in performing, there will be a large amount of text (sometimes called a \"corpus\") that we will use for training the model on. That text needs to be split up somehow into bite-sized parts to operate upon. This process is known as *tokenization*. We could try treating individual characters as tokens, or [regard entire sentences as our tokens](https://claritynlp.readthedocs.io/en/latest/developer_guide/algorithms/sentence_tokenization.html), but a common mid-point is to use *words* as tokens.  \n",
        "\n",
        "> *For a great example of a character-based neural network, see [Andrej Karpaty's Char-RNN](https://github.com/karpathy/char-rnn). \\[OPTIONAL, not required\\]\n",
        "\n",
        "The simplest -- and typically *the default* -- scheme for word-level tokenization is just to split the text at every space and at every punctuation mark. Let's try an example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQyxw7I4zpJf"
      },
      "source": [
        "So for instance, the follwing sample text:\n",
        "```\n",
        "I'm going to the store, because I need some milk.\n",
        "```\n",
        "might become\n",
        "```\n",
        "[\"I\", \"'\", \"m\", \"going\", \"to\", \"the\",  \"store\", \",\", \"because\", \"I\", \"need\", \"some\", \"milk\", \".\"]\n",
        "```\n",
        "Tokenization is something that many computational linguists have spent a great deal of time on, and there are [a variety of tokenizers](https://towardsdatascience.com/overview-of-nlp-tokenization-algorithms-c41a7d5ec4f9?gi=73a2ec14356e) available. Generally it's generally in our best interest to just call a library such as[Natural Language Toolkit (NLTK)](https://www.nltk.org/) to do the tokenizing for us instead of trying to do it from scratch. Both FastAI and HuggingFace allow us to choose between a variety of tokenizers.  (FastAI's default tokenizer is currently from the [spaCy NLP library](https://spacy.io/).)\n",
        "\n",
        "Let's try an actual example using the NLTK word tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVfwpXS703Os",
        "outputId": "b1fc7784-0d66-458e-d5ce-7461b7809c2d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')    # this is a resource needed by NLTK\n",
        "sentence = \"I'm going to the store, because I need some milk.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print(\"tokens = \",tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "tokens =  ['I', \"'m\", 'going', 'to', 'the', 'store', ',', 'because', 'I', 'need', 'some', 'milk', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSr3MFM208sK"
      },
      "source": [
        "Interesting that the apostrophe from \"I'm\" went with the \"m\" (as in \"'m\") instead of being its own thing. Presumably this is so we can then expand it into \"am\".  What about the \"n\" in \"don't\"? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWotEHBA1Onf",
        "outputId": "991bb471-7a96-42f4-a011-9708e77951f1"
      },
      "source": [
        "sentence2 = \"I don't know what's going to happen in this case, but it should be interesting!\"\n",
        "tokens = nltk.word_tokenize(sentence2)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'do', \"n't\", 'know', 'what', \"'s\", 'going', 'to', 'happen', 'in', 'this', 'case', ',', 'but', 'it', 'should', 'be', 'interesting', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVcdMXA01U7F"
      },
      "source": [
        "In this case the \"n\" from \"don't\" went with the \"'t\". Again, this best facilitates filling in the missing \"o\".  Let's try some spirited Tennessee-style language:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGrenE9u1hcd",
        "outputId": "0b6a5498-262c-4a65-a6a6-8985ebb58a14"
      },
      "source": [
        "sentence3 = \"I'm fixin' to spend $1499.95 on a new four wheeler and you ain't gonna stop me, ma!\"\n",
        "print(nltk.word_tokenize(sentence3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'m\", 'fixin', \"'\", 'to', 'spend', '$', '1499.95', 'on', 'a', 'new', 'four', 'wheeler', 'and', 'you', 'ai', \"n't\", 'gon', 'na', 'stop', 'me', ',', 'ma', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raD2F2CE2K92"
      },
      "source": [
        "Wow, it knows \"you'uns\"!  And it splits \"gonna\", presumably in preparation for a mapping to \"going\", \"to\".\n",
        "\n",
        "Do we need the commas and exclamation points though?  Maybe, maybe not.  It depends on our use case.  Sometimes other punctuation is relevant, such as hashtags and @-symbols for social media.  NLTK has a special tokenizer for Twitter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib1dPC01j24G",
        "outputId": "b895255c-365d-4428-8efd-4d791d87340a"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tt = TweetTokenizer()\n",
        "\n",
        "tweet = \"OMG I love @SuperFamousPerson's new look! #fridays #nofilter\"\n",
        "\n",
        "# Let's compare the two tokenizers:\n",
        "print(\"Regular word tokenizer:\", nltk.word_tokenize(tweet))\n",
        "print(\"Tweet tokenizer:       \",tt.tokenize(tweet))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular word tokenizer: ['OMG', 'I', 'love', '@', 'SuperFamousPerson', \"'s\", 'new', 'look', '!', '#', 'fridays', '#', 'nofilter']\n",
            "Tweet tokenizer:        ['OMG', 'I', 'love', '@SuperFamousPerson', \"'\", 's', 'new', 'look', '!', '#fridays', '#nofilter']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVd1JdCSk5mg"
      },
      "source": [
        "...So the specialty `TweetTokenizer` kept certain kinds of punctuation with their associated words, rather then splitting at all forms of punctuation like the regular word tokenizer did."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqXeh85J02ZM"
      },
      "source": [
        "\n",
        "\n",
        "Beyond the question of which punctuation to keep, we must also recognize that words come in a variety of forms.  And some words may be \"filler\" that we may not need for the task at hand (e.g., articles like \"a\", \"an\", and \"the\" are often discarded).  So we may wish to regard related words such as \"jump\", \"jumping\", \"jumps\",... as variations on the *stem* of \"jump\".  We may hang on to the endings such as \"-ing\" for later use, regarding them as additional tokens. The process of *stemming* or \"*stemmification*\" is the breaking up of words into their stems and hanging on to endings (or not).  Also, what about compound words?  Some languages such as German will make very long single words (e.g. Geschwindigkeitsbegrenzung for \"speed limit\") that in other languages would be considered as separate words. If language translation is our goal, some way of tokenizing that includes such variability would be important.  Also, what about punctuation? To keep things simple, we could just delete all forms of punctuation -- or expand contractions like \"I'll\" to \"I will\", and so forth -- and yet if we want a highly accurate model we may find that holding on to some forms of punctuation will important.\n",
        "\n",
        "\n",
        "\n",
        "####  Special Token Codes\n",
        "Often language models will make use of special tokens such as `UNK` (a token to substitute for unknown words) or `PAD` (for extra padding words), or `EOS` (end of sentence), depending on the task at hand. Sometimes these will have extra characters like `<UNK>` or `[UNK]`. There may or may not be `<START>` and `<END>` tokens for the beginning and end of the text.  The exact list of special tokens depends on the tokenizer and the model, but those few are pretty universal. So when you see those, in what follows, you'll be prepared.  \n",
        " \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5K6M4aTmWLW"
      },
      "source": [
        "## Numericalization & Word Vectors\n",
        "Once we have the tokens, we still need to convert these into numbers somehow so we can operate on them mathematically. Depending on the application, different numericalization schemes are available. \n",
        "\n",
        "One *very simple* way to do this if we were, say, doing *Sentiment Classification* in tweets, movie reviews, or other kinds of \"posts\",  would be to count the frequency of all the words that appear in positive posts, and do the same for all the negative posts.  Expressing these frequencies as fractions of the total number of words, we could then assign to each word its pair of \"positive use\" and \"negative use\" frequency values $(f_p, f_n)$ which lie in the two-dimensional [unit square](https://en.wikipedia.org/wiki/Unit_square) (shown below). These would then form the coordinates for a *word vector* of our word in its *embedding* space (i.e., the unit square in this case).  Then to classify a post, we could just take the sum of the word vectors of all the words in the post and see whether the result is more \"positive\" than \"negative\". In other words, we could ask, which region of the following embedding diagram does the mean of the word vectors in the post lie in?\n",
        "\n",
        "![img of regions of positive and negative](https://i.imgur.com/WauRtOR.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqpcM4r6tpO-"
      },
      "source": [
        "That might suffice as a simple baseline model, and it might work \"ok\", but there are issues with it. For example, it's possible that different words could get mapped to the exact same point.  If all you care about is how positive or how negative the post (or tweet, or review) is, this may not be a problem,  but if you want to \"understand\" the text, produce a translation of it, or generate new text, then this method is useless.  Another issue is that words that mean almost the same thing but are used with different frequencies (e.g. \"amazing\" and \"stupendous\") would receive very different word vectors, even though we'd want them to have essentially the same effects on the model's output.\n",
        "\n",
        "> Terminology: our simplistic method of just summing up the word vectors together pays no attention to the *order* of the words, so the above model would be termed a \"Bag of Words\" type of model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfFS04zavBVD"
      },
      "source": [
        "In order to help preserve uniqueness as well as to better allow words to express their ranges of meanings, one typically uses many more than two dimensions for word vector embeddings.  It's quite common to see 256 or more (e.g. 300) dimensions for words.  While these are too many dimensions to visualize (which is why I gave the simple example above!) the computer is able to deal with them just fine.  \n",
        "\n",
        "The way one typically gets these word vectors is to take in the list of all the (unique) words in the corpus and produce a \"vocabulary\" which indexes the words and generates a one-hot encoding by treating the words as categories.  Then we map these categories into word vectors via a matrix of trainable weights. So, for example, a corpus with 10,000 unique words mapped into 300-dimensional word vectors would involve a weights matrix of 300\\*10000 = 3 million weights. \n",
        "\n",
        "\\[TODO: Add a picture someday! ;-) \\] \n",
        "\n",
        "Thus *the \"embedding\" mapping is itself a neural network* which we train as the front-end of our full (larger) neural network.\n",
        "This means that the more words you allow in your vocabulary (or \"vocab\"), the bigger that initial embedding operation will be.  Typically, in order to keep this matrix from getting too big, one will truncate the list of words by removing the less frequent or less important words from the vocab and replacing them with special tokens such as `UNK`. \n",
        "The form the embedding takes may depend on the task.  \n",
        "\n",
        "## Language Modeling as a Pretraining Task\n",
        "One very useful method is to use a *language model* task to produce word embeddings.  A language model tries to predict the next word in a sequence given its preceding words (how many preceding words you use determines the sophistiation of the model). This forms a \"self-supervised learning\" method in the sense that the target data you train on is the same as the input data, just shifted ahead by one word. \n",
        "\n",
        "This approach was used to great effect by Jeremy Howard and Sebastian Ruder in their [ULMFit paper](https://paperswithcode.com/method/ulmfit), in which they used a language model task of predicting the next word in Wikipedia (specifically, the [Wikitext-103](https://paperswithcode.com/dataset/wikitext-103) dataset) in order to condition the model to use for other tasks such as sentiment analysis of IMDB movie reviews.  Their result was that they beat other competing sentiment analysis methods by a longshot!  \n",
        "\n",
        "The idea is that a model that has to predict the next word in a large text has to develop somewhat of an \"understanding\" of how language works, and thus will be a more powerful model for text classification than a simpler model that \n",
        "\n",
        "> Note: A neat effect of this form of pre-training is that you also end up with a text generation model.\n",
        "\n",
        "Now, we're not going to train a model on Wikipedia right now.  That would be a waste of time, as we can just download pretrained weights and go from there.  Let's use the fastai set of methods for doing this, and we'll work through their IMDB example problem [as described in Chapter 10 of the `fastbook`](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb).  To get started we'll need to download the dataset and start using fastai's tokenizer(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEjn2JVGP0Ms"
      },
      "source": [
        "!pip install -Uqq fastai fastbook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLEk7zgv41Uf"
      },
      "source": [
        "# if the next line produces an error, restart the runtime and try again.\n",
        "import fastbook  \n",
        "from fastai.text.all import *\n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIUqbBRN4nRB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "69844601-a843-41f8-ed4c-fb3016b20f9a"
      },
      "source": [
        " path = untar_data(URLs.IMDB)  # download the dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [144441344/144440600 00:02<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lp0X73dm4X3R",
        "outputId": "749db46e-5d6c-4f08-d6ea-74a4764c55d5"
      },
      "source": [
        "# make a list of all the files in all the folders of the dataset\n",
        "files = get_text_files(path, folders = ['train', 'test', 'unsup'])\n",
        "\n",
        "# let's look at the first 75 characters of the first file in the list\n",
        "txt = files[0].open().read();  txt[:75]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One True Thing rises above its potentially schlocky material to give us a v'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcU9CTZD6hG8"
      },
      "source": [
        "As we mentioned above the current default tokenizer in FastAI is from the spaCy NLP package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGshRMNu6bzS",
        "outputId": "2cee77e7-2821-452b-8f0f-4b6ebcd39e94"
      },
      "source": [
        "spacy = WordTokenizer()\n",
        "spacified = spacy([txt])  \n",
        "print(spacified)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object SpacyTokenizer.__call__.<locals>.<genexpr> at 0x7fc605448050>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeCAdqIn7K6u"
      },
      "source": [
        "So the word tokenizer is a generator. In order to access its output we can use `first()` and `next()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrcspiSf6pUx",
        "outputId": "534af5d0-bb37-4bc3-93c3-0be5a6840cdf"
      },
      "source": [
        "toks = first(spacy([txt]))\n",
        "print(toks)   # This prints out all the tokens\n",
        "print(coll_repr(toks, 30))  # fastai's coll_repr method gives the total size and first N (=30) tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'True', 'Thing', 'rises', 'above', 'its', 'potentially', 'schlocky', 'material', 'to', 'give', 'us', 'a', 'view', 'of', 'a', 'family', 'of', 'complex', 'relationships', 'and', 'flawed', ',', 'real', 'people', '.', 'It', 'opens', 'with', 'Rene', 'Zeleweger', 'discussing', 'her', 'mother', \"'s\", 'death', 'with', 'the', 'District', 'Attorney', ';', 'sparing', 'us', 'the', 'cheap', 'cinematic', 'shots', 'of', 'a', '\"', 'shocking', '\"', 'illness', 'and', 'death', '.', 'From', 'there', 'it', 'proceeds', 'into', 'a', 'look', 'at', 'a', 'family', 'system', ',', 'in', 'which', 'everyone', 'plays', 'by', 'a', 'set', 'of', 'unexamined', 'rules', ',', 'and', 'uses', 'the', 'mother', \"'s\", 'cancer', 'to', 'show', 'what', 'happens', 'when', 'all', 'the', 'rules', 'change', '.', '<', 'br', '/><br', '/>William', 'Hurt', 'as', 'the', 'self', '-', 'important', 'father', ',', 'and', 'Meryl', 'Streep', 'as', 'the', 'Suzy', 'Homemaker', 'mother', 'are', 'both', 'superb', ';', 'nuanced', 'and', 'not', 'what', 'they', 'appear', 'to', 'be', '.', 'Zeleweger', 'is', 'seething', ',', 'angry', 'and', 'surprised', 'with', 'herself', '.', 'Tom', 'Everett', 'Scott', 'does', \"n't\", 'have', 'much', 'to', 'do', ',', 'but', 'he', 'does', 'it', 'well.<br', '/><br', '/>The', 'story', 'is', 'predictable', ',', 'and', 'takes', 'at', 'least', 'one', 'badly', 'soppy', 'turn', 'it', 'need', \"n't\", 'have', 'taken', ',', 'but', 'the', 'performances', ',', 'and', 'the', 'view', 'of', 'family', 'as', 'a', 'place', 'where', 'anger', 'and', 'love', 'are', 'equally', 'mixed', ',', 'make', 'it', 'worthwhile', '.']\n",
            "(#197) ['One','True','Thing','rises','above','its','potentially','schlocky','material','to','give','us','a','view','of','a','family','of','complex','relationships','and','flawed',',','real','people','.','It','opens','with','Rene'...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOcUtcWh8Gtp"
      },
      "source": [
        "In addition to `WordTokenizer`, fastai adds some extra functionality via a `Tokenizer` method, that will turn all words to lower case but precede such interventions with a special code `xxmaj` indicating that the next word should be capitalized.  It also adds `xxbos` to denote the beginning of the sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k9AH3Sj761H",
        "outputId": "635e265c-20f4-436d-b40d-7f49d1d1e4dd"
      },
      "source": [
        "tkn = Tokenizer(spacy)\n",
        "print(coll_repr(tkn(txt), 31))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#217) ['xxbos','xxmaj','one','xxmaj','true','xxmaj','thing','rises','above','its','potentially','schlocky','material','to','give','us','a','view','of','a','family','of','complex','relationships','and','flawed',',','real','people','.','xxmaj'...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5wJRkJ18trP"
      },
      "source": [
        "> Note: fastai also has a tokenization method that will use sub-words -- i.e., groups of characters -- but we're going to skip that part for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfZq-2xc89AS"
      },
      "source": [
        "To do calculations on the GPU, it's helpful to work with \"batches\" of data, just like we did for images.  In each batch we need the same demensions, so we will chop the text up into \"chunks\" of length `seq_len` and then group these into batches.  Rather than totally randomly assigning the order of the batches, we will have the model \"read\" the text sequentially, where each new element of a batch will simply be shifted ahead one word. \n",
        "\n",
        "See this fastai example where they use a batch size of `bs=6` and sequence length of `seq_len=5` to produce one batch from a sample text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbhnhqg3-X0M",
        "outputId": "b15111c1-230f-42e9-dbac-a08b9c9daff5"
      },
      "source": [
        "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
        "print(stream)\n",
        "tokens = tkn(stream)\n",
        "print(\"\\n\",len(tokens),\"tokens in stream.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\n",
            "Then we will study how we build a language model and train it for a while.\n",
            "\n",
            " 90 tokens in stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB_E9QMY_Y29"
      },
      "source": [
        "Although we could randomly grab \"chunks\" from all over the file and try to predict the word following each chunk, the fastai folks recommend making the text in each row of each batch follow immediately from the text in the corresponding row the previous batch.  Which means making some fancy slicing code like the following, in which we show three sequential batches.  \n",
        "\n",
        "> Note: The motivation for doing this may not become clear until we define language models below (e.g. `LMModel3`) that can maintain an internal state between batches.  This internal state will be the \"glue\" that holds the sentences together in between batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "yMiGHdF67-ab",
        "outputId": "e73a1623-5b85-4bd8-93d7-91f23f3f2b19"
      },
      "source": [
        "bs,seq_len = 6, 5                          # batch size and sequence length\n",
        "num_batches = len(tokens)// bs // seq_len  # 30 tokens per batch, 90 tokens = 3 batches. \n",
        "print(\"num_batches = \",num_batches)  \n",
        "num_rows = len(tokens) // seq_len          # total rows of all batches == 18\n",
        "print(\"num_rows = \",num_rows)\n",
        "\n",
        "for b in range(num_batches):\n",
        "    stride = seq_len * num_batches \n",
        "    d_tokens = np.array([tokens[i*stride + b*seq_len :i*stride + b*seq_len + seq_len] for i in range(bs)]) # i is the row number\n",
        "    df = pd.DataFrame(d_tokens)\n",
        "    print(f\"\\nbatch = {b}:\")\n",
        "    display(HTML(df.to_html(index=False,header=None)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches =  3\n",
            "num_rows =  18\n",
            "\n",
            "batch = 0:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>movie</td>\n",
              "      <td>reviews</td>\n",
              "      <td>we</td>\n",
              "      <td>studied</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>first</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>how</td>\n",
              "      <td>to</td>\n",
              "      <td>customize</td>\n",
              "      <td>it</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>preprocessor</td>\n",
              "      <td>used</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>will</td>\n",
              "      <td>study</td>\n",
              "      <td>how</td>\n",
              "      <td>we</td>\n",
              "      <td>build</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "batch = 1:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>chapter</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>dig</td>\n",
              "      <td>deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>processing</td>\n",
              "      <td>steps</td>\n",
              "      <td>necessary</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxmaj</td>\n",
              "      <td>by</td>\n",
              "      <td>doing</td>\n",
              "      <td>this</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>data</td>\n",
              "      <td>block</td>\n",
              "      <td>xxup</td>\n",
              "      <td>api</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>a</td>\n",
              "      <td>language</td>\n",
              "      <td>model</td>\n",
              "      <td>and</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "batch = 2:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>over</td>\n",
              "      <td>the</td>\n",
              "      <td>example</td>\n",
              "      <td>of</td>\n",
              "      <td>classifying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>under</td>\n",
              "      <td>the</td>\n",
              "      <td>surface</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>convert</td>\n",
              "      <td>text</td>\n",
              "      <td>into</td>\n",
              "      <td>numbers</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>we</td>\n",
              "      <td>'ll</td>\n",
              "      <td>have</td>\n",
              "      <td>another</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>.</td>\n",
              "      <td>\\n</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>then</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>it</td>\n",
              "      <td>for</td>\n",
              "      <td>a</td>\n",
              "      <td>while</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE80zhFGFeXr"
      },
      "source": [
        "See how each row of each batch continues the text from the same row in the preceding batch?  Don't worry, you won't have to reproduce that code, fastai will do it internally.  \n",
        "\n",
        "When we were training images, we shuffled the order of images between epochs.  In the case of NLP we don't want to shuffle the words or even the rows.  Instead when we take a bunch of movie reviews and concatenate them to form a stream (which then broken into tokens and then batches), what we do is randomize the *order in which the reviews are concatenated* at each epoch.  This allows for word orderings to stay the same but where they appear in the training dataset to still shift around a bit in order to prevent overfitting. \n",
        "\n",
        "\n",
        "This is generally handled automatically by fastai, that will define the Tokenizer, set it up, and specify a Numericalize function, and set that up.  Here we show a brief example of that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "D4SGCUpz9JrL",
        "outputId": "b7409bdd-6e1c-47e2-eb50-b5e79ba21ec6"
      },
      "source": [
        "txts = L(o.open().read() for o in files[:2000])  # read texts of the first 2000 files\n",
        "txts[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One True Thing rises above its potentially schlocky material to give us a view of a family of complex relationships and flawed, real people. It opens with Rene Zeleweger discussing her mother\\'s death with the District Attorney; sparing us the cheap cinematic shots of a \"shocking\" illness and death. From there it proceeds into a look at a family system, in which everyone plays by a set of unexamined rules, and uses the mother\\'s cancer to show what happens when all the rules change. <br /><br />William Hurt as the self-important father, and Meryl Streep as the Suzy Homemaker mother are both superb; nuanced and not what they appear to be. Zeleweger is seething, angry and surprised with herself. Tom Everett Scott doesn\\'t have much to do, but he does it well.<br /><br />The story is predictable, and takes at least one badly soppy turn it needn\\'t have taken, but the performances, and the view of family as a place where anger and love are equally mixed, make it worthwhile.'"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d1ngrlbGvEv",
        "outputId": "2d2d0562-000e-4776-e5c6-7beb477d058a"
      },
      "source": [
        "toks200 = txts[:200].map(tkn)   # tokenize the first 200 files, by mapping the \"tkn\" function to the elements of text.\n",
        "toks200[0]  # show us the tokens corresponding to the text in the first file "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#217) ['xxbos','xxmaj','one','xxmaj','true','xxmaj','thing','rises','above','its'...]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YBF0Z62qHJ3V",
        "outputId": "816ad4b9-3de9-4cb5-b295-ef9708a92007"
      },
      "source": [
        "num = Numericalize()\n",
        "num.setup(toks200)   # create a vocab for the stream we've created. \n",
        "coll_repr(num.vocab,20)  # show the first 20 words in the vocab, in order of descending frequency"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"(#2016) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','it','in','i'...]\""
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA4kJ85tHpJL"
      },
      "source": [
        "Then we can show how these individual tokens are rendered as numbers.  Note that the special codes get mapped to zero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1QT3GCsHWYV",
        "outputId": "4672d816-93a9-44b3-f670-c7ed17e4cd84"
      },
      "source": [
        "nums200 = toks200.map(num);\n",
        "print(toks200[0])\n",
        "print(nums200[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['xxbos', 'xxmaj', 'one', 'xxmaj', 'true', 'xxmaj', 'thing', 'rises', 'above', 'its', 'potentially', 'schlocky', 'material', 'to', 'give', 'us', 'a', 'view', 'of', 'a', 'family', 'of', 'complex', 'relationships', 'and', 'flawed', ',', 'real', 'people', '.', 'xxmaj', 'it', 'opens', 'with', 'xxmaj', 'rene', 'xxmaj', 'zeleweger', 'discussing', 'her', 'mother', \"'s\", 'death', 'with', 'the', 'xxmaj', 'district', 'xxmaj', 'attorney', ';', 'sparing', 'us', 'the', 'cheap', 'cinematic', 'shots', 'of', 'a', '\"', 'shocking', '\"', 'illness', 'and', 'death', '.', 'xxmaj', 'from', 'there', 'it', 'proceeds', 'into', 'a', 'look', 'at', 'a', 'family', 'system', ',', 'in', 'which', 'everyone', 'plays', 'by', 'a', 'set', 'of', 'unexamined', 'rules', ',', 'and', 'uses', 'the', 'mother', \"'s\", 'cancer', 'to', 'show', 'what', 'happens', 'when', 'all', 'the', 'rules', 'change', '.', '\\n\\n', 'xxmaj', 'william', 'xxmaj', 'hurt', 'as', 'the', 'self', '-', 'important', 'father', ',', 'and', 'xxmaj', 'meryl', 'xxmaj', 'streep', 'as', 'the', 'xxmaj', 'suzy', 'xxmaj', 'homemaker', 'mother', 'are', 'both', 'superb', ';', 'nuanced', 'and', 'not', 'what', 'they', 'appear', 'to', 'be', '.', 'xxmaj', 'zeleweger', 'is', 'seething', ',', 'angry', 'and', 'surprised', 'with', 'herself', '.', 'xxmaj', 'tom', 'xxmaj', 'everett', 'xxmaj', 'scott', 'does', \"n't\", 'have', 'much', 'to', 'do', ',', 'but', 'he', 'does', 'it', 'well', '.', '\\n\\n', 'xxmaj', 'the', 'story', 'is', 'predictable', ',', 'and', 'takes', 'at', 'least', 'one', 'badly', 'soppy', 'turn', 'it', 'need', \"n't\", 'have', 'taken', ',', 'but', 'the', 'performances', ',', 'and', 'the', 'view', 'of', 'family', 'as', 'a', 'place', 'where', 'anger', 'and', 'love', 'are', 'equally', 'mixed', ',', 'make', 'it', 'worthwhile', '.']\n",
            "TensorText([   2,    8,   44,    8,  253,    8,  294,    0,  515,   95,    0,    0,  831,   15,  204,  155,   13,  472,   14,   13,  191,   14, 1485,  832,   12,    0,   10,  142,   93,   11,    8,   17,\n",
            "        1175,   32,    8,    0,    8,    0,    0,   53,  361,   23,  362,   32,    9,    8,    0,    8,    0,  148,    0,  155,    9,  979,    0,  720,   14,   13,   21,    0,   21,    0,   12,  362,\n",
            "          11,    8,   51,   65,   17,    0,  109,   13,  160,   52,   13,  191, 1486,   10,   18,   86,  254,  267,   45,   13,  306,   14,    0, 1176,   10,   12,  833,    9,  361,   23,    0,   15,\n",
            "         149,   54,  631,   76,   46,    9, 1176,  834,   11,   27,    8,  835,    8,  632,   25,    9,  633,   24,  634,  281,   10,   12,    8,    0,    8,    0,   25,    9,    8, 1177,    8,    0,\n",
            "         361,   38,  205,  721,  148,    0,   12,   37,   54,   62,  722,   15,   42,   11,    8,    0,   16,    0,   10,    0,   12,  568,   32,  836,   11,    8,  837,    8,    0,    8, 1178,   79,\n",
            "          40,   43,   97,   15,   59,   10,   30,   41,   79,   17,   85,   11,   27,    8,    9,   73,   16, 1179,   10,   12,  241,   52,  268,   44,    0,    0,  473,   17,  430,   40,   43,  980,\n",
            "          10,   30,    9,  431,   10,   12,    9,  472,   14,  191,   25,   13,  301,  143,    0,   12,  126,   38, 1487,    0,   10,  102,   17, 1488,   11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teZw3UP9H_Bo"
      },
      "source": [
        "^Note how the unknown / low frequency words get mapped to 0, which is the code for `UNK` (or \"xxunk\" in fastai parlance).\n",
        "\n",
        "These can then go into a fastai DataLoader which has been setup for language modeling, [`LMDataLoader`](https://docs.fast.ai/text.data.html#LMDataLoader), which is designed to load a batch of text as an input and the *same text shifted ahead by one word* as the target data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE8oEJW8Hnu2",
        "outputId": "c825dc3e-daba-4f08-923d-9e22378e8c29"
      },
      "source": [
        "dl = LMDataLoader(nums200)\n",
        "\n",
        "# test it\n",
        "x,y = first(dl)\n",
        "print(x.shape,y.shape)\n",
        "\n",
        "# we can print out x & y but lets convert them from numbers to text when we view them\n",
        "print(', '.join(num.vocab[o] for o in x[0][:20]))\n",
        "print(', '.join(num.vocab[o] for o in y[0][:20]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 72]) torch.Size([64, 72])\n",
            "xxbos, xxmaj, one, xxmaj, true, xxmaj, thing, xxunk, above, its, xxunk, xxunk, material, to, give, us, a, view, of, a\n",
            "xxmaj, one, xxmaj, true, xxmaj, thing, xxunk, above, its, xxunk, xxunk, material, to, give, us, a, view, of, a, family\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM15JYsCJWTX"
      },
      "source": [
        "See how each word in y is just the corresponding \"next word in x\" at the same index?  As a simple exercise, can you do the same?  Write a \"shift left\" function that just shifts a set of list elements to the left.  Add a \"xxpad\" on the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lCCCSuJIldm"
      },
      "source": [
        "## UNGRADED EXERCISE 8.0. Fill in your code below as directed\n",
        "\n",
        "def shift_left(orig:list):   \n",
        "    ## Your code below. Define a variable called \"shifted\" that is the original \n",
        "    #  list, shifted to the left by one, and filled in with a \"xxpad\" at the end.\n",
        " \n",
        "    shifted =  \n",
        " \n",
        "    ## end of your code\n",
        "    return shifted "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvAPBosZKsKE"
      },
      "source": [
        "Test your code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIYzPWbGKcQV",
        "outputId": "1dda733d-fb74-49fd-ad09-83cac5ef31f8"
      },
      "source": [
        "shift_left([1,2,3,4,5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 5, 'xxpad']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg427WeUKiEV"
      },
      "source": [
        "```\n",
        "Expected ouput:\n",
        "[2, 3, 4, 5, 'xxpad']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omqK89l_KgJl"
      },
      "source": [
        "# and another check\n",
        "assert shift_left([]) == ['xxpad']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfMqXDYjcqB_"
      },
      "source": [
        "---\n",
        "\n",
        "# Part II\n",
        "\n",
        "## More Exercises!\n",
        "\n",
        "Huggingface and fastai will end up hiding a lot of what's happening from us, so let's try writing a few more simple helper routines of our own so that we get a feel for what's involved.  The following will be graded. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF2keRTvk_XF"
      },
      "source": [
        "### Exercise 8.1: `count_freqs`\n",
        "Given a list, count up the number of times that each element appears in the list.  Return this as a Python dict called `freqs`:\n",
        "\n",
        "Note that this can be done as a one-liner using `Counter` from the builtin Python `collections` library, or you can write something similar from scratch yourself.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1fSC10lk_-r"
      },
      "source": [
        "## GRADED EXERCISE 8.1\n",
        "from collections import Counter\n",
        "\n",
        "def count_freqs(tokens:list):\n",
        "    ## YOUR CODE HERE\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "    return freqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epb81-J8myaV"
      },
      "source": [
        "Here's some code to check yourself:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D-N3bGzm0v2"
      },
      "source": [
        "test_list = ['a','b','c','a','d','z','z','q','z','b']\n",
        "freqs = count_freqs(test_list); freqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Ol1Cw6m45t"
      },
      "source": [
        "Expected output (note that your order may be different because dicts don't preserve order, but the values should be the same):\n",
        "```\n",
        "Counter({'a': 2, 'b': 2, 'c': 1, 'd': 1, 'q': 1, 'z': 3})\n",
        "```\n",
        "or\n",
        "```\n",
        "dict({'a': 2, 'b': 2, 'c': 1, 'd': 1, 'q': 1, 'z': 3})\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeAaBH9Jn7zC"
      },
      "source": [
        "# another test:\n",
        "assert freqs['z'] == 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMPyZukenLE1"
      },
      "source": [
        "### Exercise 8.2: `sort_by_freq`\n",
        "Given a list, sort its elements in **descending** order of frequency. You should call `count_freqs` in this function.  [Here's a hint](https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoSDkWTenVr_"
      },
      "source": [
        "## GRADED EXERCISE 8.2\n",
        "def sort_by_freq(tokens:list):\n",
        "    #call count_freqs to get the frequencies\n",
        "    freqs = ...\n",
        "\n",
        "    # then sort the tokens according to freqz\n",
        "    sorted_tokens = \n",
        "    \n",
        "    return sorted_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyExCLoPq62c"
      },
      "source": [
        "Test code for you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiMWx75Mq8c6"
      },
      "source": [
        "assert sort_by_freq(test_list) == ['z', 'a', 'b', 'c', 'd', 'q']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3YNeIA9k-Q-"
      },
      "source": [
        "\n",
        "### Exercise 8.3: `set_vocab_codes`\n",
        "This will be akin to the \"setup\" method of fastai's Numericalize: Given an input text,...\n",
        "\n",
        "1. Tokenize it via the defined `tokenize` method. This will give you a list we'll call `tokens`. \n",
        "2. Then rank `tokens` in decreasing order of frequency of their occurance in the text.  Call your `sort_by_freq` function for this.\n",
        "3. Truncate the list of tokens and only keep the top `keep_frac` fraction of it.\n",
        "4. Add an 'xxunk' token at the beginning of the list of tokens. \n",
        "5. Finally produce a Python `dict` called `vocab_codes` that will map tokens to their index on the sorted list. \n",
        "\n",
        "Also, make sure that any unknown words applied to `vocab_codes` return as a [default dict value](https://stackoverflow.com/questions/52195897/how-to-create-a-dict-that-can-account-for-unknown-keys) the code for `xxunk`. \n",
        "\n",
        "> Note: The fastai/spacy tokenizer is setup as a *generator*, which is not helpful for this exercise. For this reason we'll use NLTK's tokenizer instead. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gekmM_NCupVw"
      },
      "source": [
        "from fastai.text.all import *\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('punkt')    # this is a resource needed by NLTK"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xMzagpUKoJ6"
      },
      "source": [
        "## GRADED EXERCISE 8.3\n",
        "\n",
        "def set_vocab_codes(text:string, tokenizer=nltk.word_tokenize, keep_frac=0.5):\n",
        "    # INSERT YOUR OWN CODE BELOW\n",
        "    # 1. Tokenize text via the defined `tokenizer` method. This will give you a list we'll call `tokens`.\n",
        "    tokens = ...\n",
        "\n",
        "    # 2. Then rank `tokens` in decreasing order of frequency of their occurance in the text.  Call your sort_by_freq()\n",
        "    tokens = ...\n",
        "\n",
        "    # 3. Truncate the list of tokens and only keep the top `keep_frac` fraction of it.\n",
        "    tokens = ...\n",
        "\n",
        "    # 4. Add an 'xxunk' token at the beginning of the ranked list of tokens. \n",
        "    tokens = ...\n",
        "\n",
        "    # 5. Finally produce a Python `dict` called `vocab_codes` that will map tokens to their index on the sorted list. \n",
        "    vocab_codes = ...\n",
        "\n",
        "    # Also, (You may want to do this before #5) Make sure that any unknown words applied to `vocab_codes` return as a default \n",
        "\n",
        "\n",
        "    ## END OF YOUR CODE\n",
        "    return vocab_codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bHAWI90hQT_"
      },
      "source": [
        "text = 'The quick brown fox jumped over the lazy dog'\n",
        "codes = set_vocab_codes(text)\n",
        "codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DZsviv0tO9I"
      },
      "source": [
        "Expected output:    Your codes dict may have a different order than this, but the values should be the same:\n",
        "\n",
        "```\n",
        "defaultdict(<function __main__.set_vocab_codes.<locals>.<lambda>>,\n",
        "            {'The': 1, 'brown': 3, 'fox': 4, 'quick': 2, 'xxunk': 0})\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNB5FRM2tMI7"
      },
      "source": [
        "# more tests for you:\n",
        "assert codes['fox'] == 4\n",
        "assert codes['Kwisatz Haderach'] == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8LjYEA5g44n"
      },
      "source": [
        "text = 'It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.'\n",
        "codes = set_vocab_codes(text)\n",
        "codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqh28Bv-tpsW"
      },
      "source": [
        "Expected output:  (Again, the dict order may not be the same, but you should see the same values)\n",
        "\n",
        "```\n",
        "defaultdict(<function __main__.set_vocab_codes.<locals>.<lambda>>,\n",
        "            {',': 4,\n",
        "             'It': 10,\n",
        "             'age': 7,\n",
        "             'best': 11,\n",
        "             'epoch': 8,\n",
        "             'it': 5,\n",
        "             'of': 3,\n",
        "             'season': 9,\n",
        "             'the': 2,\n",
        "             'times': 6,\n",
        "             'was': 1,\n",
        "             'xxunk': 0})\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB7eMuCVt4M_"
      },
      "source": [
        "assert codes['it'] == 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_JCy5ppvmtG"
      },
      "source": [
        "You'll notice in the above example that \"It\" and \"it\" are treated as two separate words. We could send the whole text in as lowercase to get a different result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MnFJUQavnVr"
      },
      "source": [
        "codes = set_vocab_codes(text.lower())\n",
        "codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb0qwdnlvqA4"
      },
      "source": [
        "Expected output: (order may not be the same)\n",
        "```\n",
        "defaultdict(<function __main__.set_vocab_codes.<locals>.<lambda>>,\n",
        "            {',': 5,\n",
        "             'age': 7,\n",
        "             'best': 10,\n",
        "             'epoch': 8,\n",
        "             'it': 1,\n",
        "             'of': 4,\n",
        "             'season': 9,\n",
        "             'the': 3,\n",
        "             'times': 6,\n",
        "             'was': 2,\n",
        "             'worst': 11,\n",
        "             'xxunk': 0})\n",
        "```\n",
        "\n",
        "...And now \"it\" is the most frequent non-unk word in the list.   We could still do like fastai and insert a 'xxmaj' code before every capitalization, but... let's move on for now.  \n",
        "\n",
        "\n",
        "## Exercise 8.4: `codes_to_words`\n",
        "One other useful thing will be a way to convert from the codes *back* to the words themselves.  Let's create a function that will return a dict in which the keys and values have been swapped.  [Here's a hint](https://www.geeksforgeeks.org/python-program-to-swap-keys-and-values-in-dictionary/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85rID2xQxfzE"
      },
      "source": [
        "## GRADED EXERCISE 8.4 \n",
        "\n",
        "def codes_to_words(codes:dict):\n",
        "    ## YOUR CODE BELOW\n",
        "    words = ...\n",
        " \n",
        "    ## END OF YOUR CODE\n",
        "    return words "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8G9YdCcxj56"
      },
      "source": [
        "words = codes_to_words(codes)\n",
        "assert words[9] == 'season'\n",
        "words[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0-j-PHwsISa"
      },
      "source": [
        "### How We Get Word Vector Embeddings\n",
        "\n",
        "It's incredibly simple: The `dict` variables that map words to codes and codes to words serve as what computational scientists \"look up tables\". Neural networks don't exactly work with look-up tables but they can work with a very close analogue via matrix multiplication: What we do is take the word codes (/indexes) an use these to denote the rows (or columns) of a *one-hot encoding*, which is a matrix with zeros almost everywhere, and a 1 in each column & row.  The simplest version being just a diagonal matrix of 1's which can be created by functions like Numpy's `eye`.\n",
        "\n",
        "> Note: Literally, the code you'd write is simply `np.eye(len(tokens))`. It's so simple that we're not even going to make a programming assignment for it, but we *will* do a slighly trickier exercise below.\n",
        "\n",
        "Say for example we only had 3 words in our vocabulary, \"James\", \"loves\", and \"tacos\".  The one-hot encoding for these could be \n",
        "\n",
        "```\n",
        "'James' = [1, 0, 0]\n",
        "'loves' = [0, 1, 0]\n",
        "'tacos' = [0, 0, 1]\n",
        "```\n",
        "\n",
        "Then a look-up table could be created when we multiply the *matrix* of one-hot encoded words (i.e.  `np.eye(3)` in this case) with whatever number we want. \n",
        "\n",
        "The \"number we want\" will be a set of numbers, in the form of *weights* of a neural network `Linear` layer (with no activation, i.e. linear activation).  This will produce our word embeddings!  To be clear: the weights will be the same as the activations because we are multiplying \"1\" by the weights and using no activation.  The dimension of the word vectors produced will be determined by how many dimensions we want in this Linear layer of weights -- for example, we mentioned 300 before.\n",
        "\n",
        "These weights (i.e. the word embeddings themselves) are initialized randomly and *learned* in the context of training the model for whatever task we want.\n",
        "\n",
        "You might wonder: is using just one Linear layer with no activation sufficient to accurately map out human language to the point fo being able to produce meaningful embeddings?  In practice, this is remarkably effective, and in fact the `nn.Embedding` layers in both PyTorch and Jupyter are literally just one-hot encoders attached to Linear layers (in Keras they're called \"Dense\" layers).  \n",
        "\n",
        "How useful will these embeddings be?  Well, that's an interesting question. \"Universal\" word embedding such as Word2Vec of GloVe are trained on huge datasets in order to be as general as possible, whereas if you were to simply train on a very small dataset, your embeddings might be only useful for the specific task you want.  Generally, it's useful to start with pretrained embeddings in a \"frozen\" (non-trainable) state as you train the downstream part of your neural network, and then gradually \"unfreeze\" the network starting fro the later layers and working backward was the model trains.  (The ULMFiT method referenced earlier describes a detailed way of doing this.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiA6hnBSsNmf"
      },
      "source": [
        "### Exercise 8.5: `token_to_one_hot`\n",
        "In this example, we're not going to encode *all* the available tokens at once, rather we're just going to produce the one-hot encodings of the particular words one might find in a sequence.  This will simply involve using the `codes` dict you created to get the index of a word and then forming a one-hot version of that word -- i.e. all zeros except for a 1 at the element corresponding to the word's index/code. The length of the one-hot vector will be the total number of possible words.  So for example, if we had 1000 words, the 'xxunk' would be one-hot encoded as a 1 in the first (0th) spot followed by a list of 999 zeros.  \n",
        "\n",
        "In the following, use `torch.zeros()` to initialize the vector, with a length of `len(codes)`.\n",
        "\n",
        "***As an additional requirement: Your routine should return the one-hot vector for 'xxunk' for any token not already assigned a code.*** (You may use recursion to achieve this if you like.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sItFtt2VsLDi"
      },
      "source": [
        "## GRADED EXERCISE 8.5\n",
        "import torch \n",
        "def token_to_one_hot(token, codes):\n",
        "    ### Your code below. Produce a 1D numpy array corresponding to the one-hot vector for token\n",
        "\n",
        "\n",
        "    ### end of your code\n",
        "    return onehot_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoRmBwrmsQib"
      },
      "source": [
        "# a little test code for you\n",
        "from collections import defaultdict \n",
        "test_codes = defaultdict(lambda x:0) \n",
        "for key, value in {'xxunk':0, 'the':1, 'apples':2, 'are':3, 'tasty':4}.items():\n",
        "    test_codes[key] = value\n",
        "\n",
        "print(token_to_one_hot('apples', test_codes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3kXUhbasSh0"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "    tensor([0., 0., 1., 0., 0.])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IMAmOyLsTLU"
      },
      "source": [
        "# More tests:\n",
        "assert torch.equal( token_to_one_hot('tasty',test_codes), torch.Tensor([0,0,0,0,1]) )\n",
        "\n",
        "# defaultdict will not automatically handle this next one! Your routine will need to catch it\n",
        "assert torch.equal( token_to_one_hot('smorgasbord',test_codes), torch.Tensor([1,0,0,0,0]) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1JMZgh0sW8J"
      },
      "source": [
        "Now, if our \"batch\" is going to have sequences of words, and each word is a one-hot vector, then won't we end up having a 3-dimensional array for our input?  That was fine for images because we were using 2D convolution operations.  How will we structure the one-hot encodings in our word vectors -- as rows or columns?\n",
        "\n",
        "Let's try a simple example.  Pretend these words are the tokens. We're going to want to process each word and produce its word vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOGADY0FsUcc"
      },
      "source": [
        "test_batch = [['here','is','a','sequence'],['and','here','is','another'],['one','more','sequence','here']]\n",
        "\n",
        "# Uhhh ok actually our set_vocab_codes needs a string, so we'll convert to an array and then flatten it\n",
        "batch_array = np.array(batch)\n",
        "print(\"batch_array = \\n\",batch_array)\n",
        "print(\"batch_array.shape =\",batch_array.shape)\n",
        "test_text = ' '.join( batch_array.flatten().tolist() )\n",
        "print(\"test_text = \",test_text)  # ok, now we've got our string to send to set_vocab_codes\n",
        "\n",
        "test_codes = set_vocab_codes( test_text )\n",
        "print(\"test_codes =\",test_codes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpOqHEAnsefe"
      },
      "source": [
        "Now we'll produce the one-hot encodings for this batch of sequences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvsee_oysfW-"
      },
      "source": [
        "onehot_batch = torch.zeros( (batch_array.shape[0], batch_array.shape[1], len(test_codes)) )\n",
        "\n",
        "for i, row in enumerate(batch_array):\n",
        "    for j, word in enumerate(row):\n",
        "        onehot_batch[i, j] = token_to_one_hot(word, test_codes)\n",
        "\n",
        "(print(onehot_batch) # here the onehot encodings of words will appear along rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vu4jmzdsgtp"
      },
      "source": [
        "(^Coulda had you do that as an exercise, but imagined it might get confusing. ;-) )\n",
        "\n",
        "That batch of inputs will then be matrix-multiplied by a set of weights in the Linear layer of a PyTorch model we'll define as we go forward. \n",
        "\n",
        "The *output* of that PyTorch model will be a single word, namely a one-hot vector for the next word in the sequence (given by our `shift_left`-ed target data. We will use `softmax` activation and a categorical cross-entropy loss function for this since it's effectively the same thing as predicting one of a variety of categories.  These are just the multi-category versions of sigmoid and binary cross-entropy we saw before. \n",
        "\n",
        "> Note: Or even better, we can get a bit more numerical precision if we *don't* use softmax and cross-entropy explicitly but rather use the PyTorch forms of these functions that will avoid any funny exponential blowups (as discussed in our [Santa Claus example](https://hedges.belmont.edu/naughty/) for binary classification). The PyTorch `nn.CrossEntropyLoss` actually expects pre-softmax \"logit\" values so we won't apply the softmax in our model.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEitxbh5skCi"
      },
      "source": [
        "---\n",
        "\n",
        "# Part III\n",
        "\n",
        "### At this point, we've covered all the moving parts of the system except for the model itself!  \n",
        "We can swap in a variety of models, but for definiteness we're going to use what's called \"Recurrent Neural Network\" (RNN) that will retain some \"memory\" of previous \"states\" when it looked at earlier sequences.  (And *this* is why we made the requirement earlier of having one batch feed directly into the next -- it's because of of this stateful memory).  \n",
        "\n",
        "The particular form of RNN we'll use is called an LSTM (which stands for \"Long Short-Term Memory\") and it has a few nice properties for not just remembering things but for \"forgetting\" things too, and even for \"deciding\" what's worth remembering or forgetting!\n",
        "\n",
        "I want to strike a balance between writing our own code and developing a full model from scratch vs. interfacing with more powerful packages such as PyTorch and fastai.  So, now that we have a better idea of what's going on \"under the hood,\" in order to continue efficiently without having to write a bunch more code, we'll switch over to the PyTorch and fastai libraries (which do similar work as we have but also a whole lot more) so that we can take advantage of all the other \"goodies\" that these packages provide. \n",
        "\n",
        "> **Attribution:** **In what follows, we will follow a combination of [Chapter 10](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb) and [Chapter 12](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb) from the fastai \"fastbook\"**, almost verbatim.  After all, *is* listed as one of the textbooks for the course. ;-)   We will however change up the order, covering the Chapter 12 content before the Chapter 10 content. (You don't need to read these separately for what follows; I'm just citing my sources.)\n",
        "\n",
        "## The Plan\n",
        "What they do is, first load in a dataset and define their data loaders, then they define and train (from scratch) a series of RNN models ordered by increasing sophistication, finally building up to the LSTM model. (This will be the part from fastbook Chapter 12). \n",
        "In order for this proceed quickly rather than taking hours or days to train, they do this will a small text dataset called \"Human Numbers\".  \n",
        "\n",
        "We would love to then be able to use our newly-trained models to demonstrate how language-model-pretraining can help improve performance on text classification, **however the complexity of real human language demands that we employ larger datasets and larger models which then take a lot longer to train** -- in effect, we would have to reproduce the entire [ULMFiT paper](https://arxiv.org/abs/1801.06146) which was trained on WikiText-103.\n",
        "\n",
        "> **Ethics Advisory:** Language Models have become bigger and bigger in recent years (notably, [GPT-3](https://towardsdatascience.com/gpt-3-a-complete-overview-190232eb25fd) with its 175 billion paraemeters) in order to address more and more complexity of language. The cost of training these models is felt both economically and *environmentally*: burning all that energy [incurs a significant carbon footprint](https://www.theregister.com/2020/11/04/gpt3_carbon_footprint_estimate/).  Thus, we can see that in practical cases where we want sophisticated, accurate results that require large models and large datasets (rather than simple examples for teaching purposes),  Transfer Learning (i.e. starting from a pre-trained model and then fine-tuning on a smaller domain-specific dataset) rather than training from scratch becomes *not only the expedient choice but also the ethically sounder choice*. \n",
        "\n",
        "In plainer language: You don't want to pay the money and wait around to train your own sophisticated model from scratch, and we don't want to burn all that energy anyway,...but the very simple models we're going to start with for illustrative purposes won't work for what we ultimately want to do. So when it comes time to do the text classification, we will switch over (to Chapter 10 content) and load the fastai checkpoints that were already trained on Wikitext.  Think of it like a \"cooking show\" on TV, where they come back from commercial and everything's already cooked! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8NarPrBEY7g"
      },
      "source": [
        "\n",
        "### The Data\n",
        "The fastai \"Human Numbers\" dataset is just a list of the first 10,000 numbers written out sequentially in plain English.  It's not sufficient for learning the nuanaces of all of English, but it's a good demo for how a language model can learn to predict what comes next.  Let's take a look"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MWM7HR_FXSD"
      },
      "source": [
        "> **IMPORTANT:** For what follows, make sure you have GPU acceleration enabled.  Go to `Edit > Notebook settings > Hardware acclerator > GPU`.   This may reset your runtime, in which case you'll need to re-do the install * imports from the top of the notebook -- but I've gone ahead and repeated those lines below so you don't scroll up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0x17JjMEnvp",
        "outputId": "c83235b9-1fc0-419d-9271-1cfe70ae174f"
      },
      "source": [
        "# We'll make this so that you can restart the notebook from here instead of having to scroll up\n",
        "!pip install -Uqq fastai fastbook"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |                              | 10 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |                            | 20 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |                          | 30 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |                         | 40 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |                       | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |                     | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |                   | 71 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |                  | 81 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |                | 92 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |              | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |            | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |           | 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |         | 133 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |       | 143 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |     | 153 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |   | 163 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |  | 174 kB 5.5 MB/s eta 0:00:01\r\u001b[K     || 184 kB 5.5 MB/s eta 0:00:01\r\u001b[K     || 186 kB 5.5 MB/s \n",
            "\u001b[K     || 720 kB 48.4 MB/s \n",
            "\u001b[K     || 56 kB 3.1 MB/s \n",
            "\u001b[K     || 46 kB 2.8 MB/s \n",
            "\u001b[K     || 1.2 MB 47.8 MB/s \n",
            "\u001b[K     || 51 kB 351 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKt6xpI8FQ4e"
      },
      "source": [
        "# if the next line produces an error, restart the runtime and try again.\n",
        "import fastbook  \n",
        "from fastai.text.all import *\n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OaCyPT0FFtT"
      },
      "source": [
        "path = untar_data(URLs.HUMAN_NUMBERS)  # download the dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqW_Z-ByFEe_",
        "outputId": "456ee34e-97b9-4a96-f626-2fdcee788b97"
      },
      "source": [
        "lines = L()   # L() is fastai's \"super\" list class. It's like a regular list, but can do more. it's built on Numpy\n",
        "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
        "with open(path/'valid.txt') as f: lines += L(*f.readlines())\n",
        "print(lines[100:125]) # show somewhere in the middle\n",
        "lines  # and show the beginning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['one hundred one \\n', 'one hundred two \\n', 'one hundred three \\n', 'one hundred four \\n', 'one hundred five \\n', 'one hundred six \\n', 'one hundred seven \\n', 'one hundred eight \\n', 'one hundred nine \\n', 'one hundred ten \\n', 'one hundred eleven \\n', 'one hundred twelve \\n', 'one hundred thirteen \\n', 'one hundred fourteen \\n', 'one hundred fifteen \\n', 'one hundred sixteen \\n', 'one hundred seventeen \\n', 'one hundred eighteen \\n', 'one hundred nineteen \\n', 'one hundred twenty \\n', 'one hundred twenty one \\n', 'one hundred twenty two \\n', 'one hundred twenty three \\n', 'one hundred twenty four \\n', 'one hundred twenty five \\n']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgujlGXHGWuH"
      },
      "source": [
        "Turn that list into one long string, and then tokenize the string by splting at spaces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuSQWgjTGbnS",
        "outputId": "4480e0f9-d416-452a-ce23-f82b560a8a02"
      },
      "source": [
        "text = ' . '.join([l.strip() for l in lines])\n",
        "print(text[:100])\n",
        "tokens = text.split(' ')\n",
        "print(tokens[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo\n",
            "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mFJVNQwGouc"
      },
      "source": [
        "To make our vocab and numericalize it into codes/indexs, the `L()` class has a handy `.unique()` method.  You'll see that there are only 30 different unique tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5OvJ47bGqjF",
        "outputId": "c15e6220-4d68-43f8-c1fc-de695a6deb50"
      },
      "source": [
        "vocab = L(*tokens).unique()\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaR-Y6XYHCk3"
      },
      "source": [
        "The dict for mapping words to codes/indexes goes like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk6HONpDHG4S",
        "outputId": "a27c9c98-1158-4c24-cc73-99ea4a2d84f5"
      },
      "source": [
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "nums = L(word2idx[i] for i in tokens)\n",
        "nums"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTvTvxZqHVSl"
      },
      "source": [
        "For starters, we're going to use a model that only looks at sequences of 3 words at a time, so we'll produce a series of 3-element \"windows\" of the numericalized text:  (Note that period '.' gets the index of 1 so it shows up a lot.  We might be tempted to remove these, but they serve as separators between our numbers expressed as natural language.)  First, here's what that looks like in terms of words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTgtOxI7S_zz",
        "outputId": "51d79196-5ba5-4ea2-fee4-4d42fa0993f8"
      },
      "source": [
        "L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four', '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'], '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.', 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.', 'fifteen', '.'], 'sixteen')...]"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W058qInwTFtL"
      },
      "source": [
        "Then here's what it looks like in terms of numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrEa_tw-HfMD",
        "outputId": "510e327f-6e45-461d-ff5c-61fa79cfba52"
      },
      "source": [
        "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\n",
        "seqs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnVHjRjIKdlS"
      },
      "source": [
        "In what follows, we're going to try to compare things against each other, so it's helpful to set the seed of the random number generate (RNG) every time.  In fastai & PyTorch there are a lot of different random numbers being generated so as a convenience, we're going to define [the following routine](https://forums.fast.ai/t/solved-reproducibility-where-is-the-randomness-coming-in/31628/5?u=drscotthawley) to set them all at once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB1DSoueKw6v"
      },
      "source": [
        "def set_seed(dls, seed=1):   # must have dls, as it has an internal random.Random\n",
        "    # code from https://forums.fast.ai/t/solved-reproducibility-where-is-the-randomness-coming-in/31628/5\n",
        "    random.seed(seed)\n",
        "    dls.rng.seed(seed) #added this line\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlUg5IJqH_12"
      },
      "source": [
        "Fastai uses the DataLoader class to feed into its Learner class, so let's define a DataLoader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFIPnpTJIFfY"
      },
      "source": [
        "bs = 64  # batch size of 64 will work for this small data\n",
        "cut = int(len(seqs) * 0.8)    # cut will split into training and validation sets\n",
        "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)\n",
        "set_seed(dls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2fBT-SvEXWA"
      },
      "source": [
        "\n",
        "\n",
        "### The First Model\n",
        "Let's take a look at the simplest model they consider first. It's designed to take in three words at a time, which appear in the `.forward()` part of the class as `x[:,0]`, `x[:,1]`, and `x[:,2]`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_5I_duFsiQE"
      },
      "source": [
        "class LMModel1(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "        \n",
        "    def forward(self, x):  # SHH: I slightly edited this from fastbook to make it more similar to LMModel2 below\n",
        "        h = 0\n",
        "        h = h + self.i_h(x[:,0])\n",
        "        h = F.relu(self.h_h(h))\n",
        "        h = h + self.i_h(x[:,1])\n",
        "        h = F.relu(self.h_h(h))\n",
        "        h = h + self.i_h(x[:,2])\n",
        "        h = F.relu(self.h_h(h))\n",
        "        return self.h_o(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92whG_iu9IXb"
      },
      "source": [
        "So we've got a `nn.Embedding` layer, which as we said earlier is just a (very efficient) proxy for one-hot encoding paired with a linear layer.\n",
        "After that it's just a 3-layer network, but one in which we add the embeddings (`i_h`) of each word to the \"hidden state\" (`h`) of the network for this 3-word sequence.  This network has no \"memory\"; we'll add that later. \n",
        "\n",
        "Let's train this model, and look at the accuracy over a few epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImiqQ_wW-wOm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "a4bab9fc-f926-40df-ef04-f4968da96cfb"
      },
      "source": [
        "set_seed(dls)     # for comparison with the next model's run\n",
        "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, \n",
        "                metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.789529</td>\n",
              "      <td>2.008283</td>\n",
              "      <td>0.473972</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.381144</td>\n",
              "      <td>1.828828</td>\n",
              "      <td>0.468743</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.419095</td>\n",
              "      <td>1.656361</td>\n",
              "      <td>0.490611</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.404740</td>\n",
              "      <td>1.676169</td>\n",
              "      <td>0.447112</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x_PtDXJIp7B"
      },
      "source": [
        "An Accuracy of 0.49 is better than randomly guessing among 30 possible tokens (which would be a score of 0.033), but not nearly as good as we'll be able to get. Thinking a little more carefully, we might inspect the dataset itself and see what the most common token is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzR4MG5zI8xN",
        "outputId": "710ffdce-8203-4b2a-937d-3e38dca61983"
      },
      "source": [
        "n,counts = 0,torch.zeros(len(vocab))\n",
        "for x,y in dls.valid:\n",
        "    n += y.shape[0]\n",
        "    for i in range_of(vocab): counts[i] += (y==i).long().sum()\n",
        "idx = torch.argmax(counts)\n",
        "idx, vocab[idx.item()], counts[idx].item()/n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(29), 'thousand', 0.15165200855716662)"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwp2vLcTJIaw"
      },
      "source": [
        "We might expect \"thousand\" to be rather common, since 90% of the dataset has the word \"thousand\" in it, but the low-value integers like \"one\", \"two\", \"three\" and so on also appear a lot.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9aP0cNw-82O"
      },
      "source": [
        "\n",
        "### Exercise 8.6: The Second Model: `LMModel2`  ( = `LMModel1` but with a loop )\n",
        "Let's make a model that is *exactly the same as model 1* but with the `.forward()` method written using a loop.  Can you do it yourself -- without peeking at the fastbook version?  There are two empty lines in the model code below for you to fill in your own code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezCTTXkZ_A2n"
      },
      "source": [
        "## UNGRADED EXERCISE 8.6  (this will not be graded, just do it on your own)\n",
        "class LMModel2(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = 0\n",
        "        for i in range(3): \n",
        "            ### YOUR CODE BELOW: fill in the TWO LINES needed to perform the same \n",
        "            # operations as LMModel1 (but using a loop instead)\n",
        "\n",
        "            ### END YOUR CODE.\n",
        "        return self.h_o(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlN8pQEKJ1Ot"
      },
      "source": [
        "Train the network below. It should give the same numbers as above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg3r35hCAD7O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "9507f53a-c053-472b-e74f-9d13621414bb"
      },
      "source": [
        "set_seed(dls)   # for reproducibility / comparison with LMModel1's results above\n",
        "learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n",
        "                metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.789529</td>\n",
              "      <td>2.008283</td>\n",
              "      <td>0.473972</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.381144</td>\n",
              "      <td>1.828828</td>\n",
              "      <td>0.468743</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.419095</td>\n",
              "      <td>1.656361</td>\n",
              "      <td>0.490611</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.404740</td>\n",
              "      <td>1.676169</td>\n",
              "      <td>0.447112</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxc2Nel1_R7I"
      },
      "source": [
        "### The Third Model\n",
        "Now we finally come to a model that has some \"memory\": the state `h` is going to persist inside the class (as `self.h`) from one `.forward()` call to the next instead of being reset to 0 every time.  They do include a method to reset the state but it has to be called explicitly, otherwise `self.h` remains whatever value it had *last time* the forward method was called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dob9R42q_dsQ"
      },
      "source": [
        "class LMModel3(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "        self.h = 0                   # this is the \"memory\" part. self.h persists with the class after .forward() is finished\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for i in range(3):           # what follows is same as LMModel2 but with \"h\" replaced by \"self.h\"\n",
        "            self.h = self.h + self.i_h(x[:,i])\n",
        "            self.h = F.relu(self.h_h(self.h))\n",
        "        out = self.h_o(self.h)\n",
        "        self.h = self.h.detach()  # for next time: we'll keep the value of h but discard its extra gradient info\n",
        "        return out\n",
        "    \n",
        "    def reset(self): self.h = 0     # we'll call this at the beginning of a new set of text."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Wsmt5QMNuK"
      },
      "source": [
        "In order to take advantage of the state of the network, we need to make sure the \"rows\" of each batch line up across the batch boundaries.  (Remember when we wrote out those three batches of IMDB text, above?)\n",
        "\n",
        "Currently our dataset isn't set up to do this, so we're going to change it and create a new dataset of \"chunks\" that follow one another:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM426fZmNRcr",
        "outputId": "1d44dc88-4bc1-4d32-ba78-a2ad607eb4c9"
      },
      "source": [
        "m = len(seqs)//bs  # m will be the number of chunks\n",
        "m,bs,len(seqs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(328, 64, 21031)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIUvF5L6NbxV"
      },
      "source": [
        "def group_chunks(ds, bs):\n",
        "    m = len(ds) // bs\n",
        "    new_ds = L()\n",
        "    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))\n",
        "    return new_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42vPPjtyNeuK"
      },
      "source": [
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(\n",
        "    group_chunks(seqs[:cut], bs), \n",
        "    group_chunks(seqs[cut:], bs), \n",
        "    bs=bs, drop_last=True, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcM3O_loNjZN"
      },
      "source": [
        "Now we can train model 3.  The first few epochs actually won't show an advantage, so we'll train it loger:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcZroFzLAUjA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "8a1036eb-97de-471c-cbc8-e9f1446a8f71"
      },
      "source": [
        "set_seed(dls)\n",
        "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n",
        "                metrics=accuracy, cbs=ModelResetter)  # ModelResetter callback will call our model's .reset() \n",
        "learn.fit_one_cycle(10, 3e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.703804</td>\n",
              "      <td>1.852335</td>\n",
              "      <td>0.431971</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.291301</td>\n",
              "      <td>1.858340</td>\n",
              "      <td>0.426683</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.105003</td>\n",
              "      <td>1.684123</td>\n",
              "      <td>0.467308</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.996649</td>\n",
              "      <td>1.709893</td>\n",
              "      <td>0.545673</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.948535</td>\n",
              "      <td>1.794689</td>\n",
              "      <td>0.550481</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.891645</td>\n",
              "      <td>1.726101</td>\n",
              "      <td>0.571875</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.889067</td>\n",
              "      <td>1.504990</td>\n",
              "      <td>0.569471</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.823918</td>\n",
              "      <td>1.686259</td>\n",
              "      <td>0.573798</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.788420</td>\n",
              "      <td>1.686563</td>\n",
              "      <td>0.599519</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.772840</td>\n",
              "      <td>1.679055</td>\n",
              "      <td>0.599279</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG-yjqFsNwIr"
      },
      "source": [
        "Better than before!  But it's a slightly different dataset split of course we also trained it longer, and you'll notice we even bumped up the learning rate. \n",
        "\n",
        "Just so we're not comparing \"apples and oranges\", let's re-run LMModel1 on this modified dataset for the same number of epochs to see how it compares to LMModel3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "LI0C4cehN7QK",
        "outputId": "4bc45cf7-47cc-430e-820a-2f9310e46b30"
      },
      "source": [
        "set_seed(dls)\n",
        "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy,\n",
        "                metrics=accuracy)\n",
        "learn.fit_one_cycle(10, 3e-3) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.715092</td>\n",
              "      <td>1.878626</td>\n",
              "      <td>0.405769</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.385061</td>\n",
              "      <td>1.949975</td>\n",
              "      <td>0.356010</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.334724</td>\n",
              "      <td>2.018960</td>\n",
              "      <td>0.356731</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.317794</td>\n",
              "      <td>2.040227</td>\n",
              "      <td>0.358894</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.308069</td>\n",
              "      <td>2.021837</td>\n",
              "      <td>0.361779</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.302087</td>\n",
              "      <td>2.073891</td>\n",
              "      <td>0.371635</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.297517</td>\n",
              "      <td>2.083388</td>\n",
              "      <td>0.372356</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.294183</td>\n",
              "      <td>2.077134</td>\n",
              "      <td>0.353846</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.289541</td>\n",
              "      <td>2.128820</td>\n",
              "      <td>0.327163</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.284398</td>\n",
              "      <td>2.165285</td>\n",
              "      <td>0.325721</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbz2m3P4AdEn"
      },
      "source": [
        "Notice how much worse this one was than the stateful model.\n",
        "\n",
        "How can we improve on our results (LMModel3)?  For the datasets used in above examples we've actually been \"skipping\" 3 words at a time. We'd get a lot more training data (or \"signal\" as the fastbook describes) it if we moved through the dataset one word at a time.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBt10YEMJ-4J"
      },
      "source": [
        "sl = 3     # sl is the sequence length, which we'll increase from 3 in just a bit\n",
        "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
        "         for i in range(0,len(nums)-sl-1,sl))\n",
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
        "                             group_chunks(seqs[cut:], bs),\n",
        "                             bs=bs, drop_last=True, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctdy0kskSpgN"
      },
      "source": [
        "Here's an example of the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQYP1NvnSrex",
        "outputId": "802a8527-56c7-404a-875d-4088bd659372"
      },
      "source": [
        "[L(vocab[o] for o in s) for s in seqs[0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(#3) ['one','.','two'], (#3) ['.','two','.']]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGN5PQ8iTNnc"
      },
      "source": [
        " Notice how the second element reads `['.','two','.']` instead of the `['.','three','.']` that we had ^^up above a ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eh3-bjyKSlW"
      },
      "source": [
        "We need to modify our model slightly so that it can handle the change we just made. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q525bzenAYvn"
      },
      "source": [
        "class LMModel4(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, sl=3):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)  \n",
        "        self.h_h = nn.Linear(n_hidden, n_hidden)     \n",
        "        self.h_o = nn.Linear(n_hidden,vocab_sz)\n",
        "        self.h = 0\n",
        "        self.sl = sl\n",
        "        \n",
        "    def forward(self, x):\n",
        "        outs = []\n",
        "        for i in range(self.sl):  \n",
        "            self.h = self.h + self.i_h(x[:,i])\n",
        "            self.h = F.relu(self.h_h(self.h))\n",
        "            outs.append(self.h_o(self.h))\n",
        "        self.h = self.h.detach()\n",
        "        return torch.stack(outs, dim=1)\n",
        "    \n",
        "    def reset(self): self.h = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxnJwIb9Tu17"
      },
      "source": [
        "From fastbook: \"This model will return outputs of shape `bs x sl x vocab_sz` (since we stacked on dim=1). Our targets are of shape `bs x sl`, so we need to flatten those before using them in F.cross_entropy:\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UCcG2l8T2BF"
      },
      "source": [
        "def loss_func(inp, targ):\n",
        "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40BoJIEDTfRV"
      },
      "source": [
        "Let's train on that sequence length of 3, and keep training longer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZJXSrFiAhns",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "5205c954-7088-4a21-e4cf-730766fdc96c"
      },
      "source": [
        "learn = Learner(dls, LMModel4(len(vocab), 64, sl=sl), loss_func=loss_func,\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.844128</td>\n",
              "      <td>1.861560</td>\n",
              "      <td>0.466506</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.450525</td>\n",
              "      <td>1.947844</td>\n",
              "      <td>0.400721</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.407147</td>\n",
              "      <td>2.097088</td>\n",
              "      <td>0.346394</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.366056</td>\n",
              "      <td>2.111259</td>\n",
              "      <td>0.389744</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.306964</td>\n",
              "      <td>2.129084</td>\n",
              "      <td>0.392228</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.287480</td>\n",
              "      <td>2.061908</td>\n",
              "      <td>0.370753</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.290272</td>\n",
              "      <td>2.078658</td>\n",
              "      <td>0.449038</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.290166</td>\n",
              "      <td>2.450740</td>\n",
              "      <td>0.463141</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.275506</td>\n",
              "      <td>2.148760</td>\n",
              "      <td>0.477885</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.227539</td>\n",
              "      <td>2.292798</td>\n",
              "      <td>0.489984</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.201554</td>\n",
              "      <td>2.422386</td>\n",
              "      <td>0.481891</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.185501</td>\n",
              "      <td>2.338204</td>\n",
              "      <td>0.461058</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.148669</td>\n",
              "      <td>2.371628</td>\n",
              "      <td>0.474920</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.135188</td>\n",
              "      <td>2.340909</td>\n",
              "      <td>0.476202</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.132959</td>\n",
              "      <td>2.335438</td>\n",
              "      <td>0.469471</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5V5IfM0Tgj1"
      },
      "source": [
        "And now let's bump up the sequence length to 16:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "Gi2Tz7zuTlai",
        "outputId": "80651a6d-9647-4de7-9598-a23a6605c21c"
      },
      "source": [
        "sl = 16     # sl is the sequence length, which we'll increase from 3 in just a bit\n",
        "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))\n",
        "         for i in range(0,len(nums)-sl-1,sl))\n",
        "cut = int(len(seqs) * 0.8)\n",
        "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),\n",
        "                             group_chunks(seqs[cut:], bs),\n",
        "                             bs=bs, drop_last=True, shuffle=False)\n",
        "\n",
        "\n",
        "# we won't bother setting the random seed since the dataset will be different\n",
        "learn = Learner(dls, LMModel4(len(vocab), 64, sl=sl), loss_func=loss_func,\n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.116494</td>\n",
              "      <td>2.964314</td>\n",
              "      <td>0.184896</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.225303</td>\n",
              "      <td>1.936567</td>\n",
              "      <td>0.453451</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.703538</td>\n",
              "      <td>1.771978</td>\n",
              "      <td>0.483724</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.444374</td>\n",
              "      <td>1.841520</td>\n",
              "      <td>0.504883</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.259405</td>\n",
              "      <td>2.190436</td>\n",
              "      <td>0.531738</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.130306</td>\n",
              "      <td>2.082433</td>\n",
              "      <td>0.572510</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.009578</td>\n",
              "      <td>2.410586</td>\n",
              "      <td>0.590820</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.911287</td>\n",
              "      <td>2.603492</td>\n",
              "      <td>0.603841</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.827644</td>\n",
              "      <td>2.804047</td>\n",
              "      <td>0.614095</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.759426</td>\n",
              "      <td>2.857082</td>\n",
              "      <td>0.632487</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.707573</td>\n",
              "      <td>2.924228</td>\n",
              "      <td>0.628906</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.671375</td>\n",
              "      <td>2.873533</td>\n",
              "      <td>0.634196</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.643230</td>\n",
              "      <td>2.958962</td>\n",
              "      <td>0.642171</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.625061</td>\n",
              "      <td>2.962040</td>\n",
              "      <td>0.645426</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.613488</td>\n",
              "      <td>2.942244</td>\n",
              "      <td>0.646403</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRfl0VNMAj88"
      },
      "source": [
        "Ooh and that's even better, but can we still improve it?  What about adding more layers to the network?  \"Deeper\" learning should be better, right? ;-) \n",
        "Let's use PyTorch's [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) class to help us make an even deeper model, where we can specify the depth via `n_layers`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1enycl9zAie6"
      },
      "source": [
        "class LMModel5(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h = torch.zeros(n_layers, bs, n_hidden)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        res,h = self.rnn(self.i_h(x), self.h)\n",
        "        self.h = h.detach()\n",
        "        return self.h_o(res)\n",
        "    \n",
        "    def reset(self): self.h.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JkFpWZ3AzIC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "070e242c-6514-4407-9d94-5af7044774fc"
      },
      "source": [
        "learn = Learner(dls, LMModel5(len(vocab), 64, 2), \n",
        "                loss_func=CrossEntropyLossFlat(), \n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.047085</td>\n",
              "      <td>2.589521</td>\n",
              "      <td>0.440023</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.160752</td>\n",
              "      <td>1.784279</td>\n",
              "      <td>0.471273</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.712880</td>\n",
              "      <td>1.852068</td>\n",
              "      <td>0.331706</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.504376</td>\n",
              "      <td>1.861352</td>\n",
              "      <td>0.355469</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.329749</td>\n",
              "      <td>1.794908</td>\n",
              "      <td>0.462891</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.152756</td>\n",
              "      <td>1.793787</td>\n",
              "      <td>0.476400</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.997835</td>\n",
              "      <td>1.835657</td>\n",
              "      <td>0.503499</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.871044</td>\n",
              "      <td>1.938171</td>\n",
              "      <td>0.510579</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.771980</td>\n",
              "      <td>1.946762</td>\n",
              "      <td>0.529378</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.689576</td>\n",
              "      <td>2.003894</td>\n",
              "      <td>0.537028</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.630481</td>\n",
              "      <td>2.015681</td>\n",
              "      <td>0.542643</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.584797</td>\n",
              "      <td>2.089362</td>\n",
              "      <td>0.532633</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.553214</td>\n",
              "      <td>2.124773</td>\n",
              "      <td>0.532471</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.533930</td>\n",
              "      <td>2.096233</td>\n",
              "      <td>0.538086</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.523803</td>\n",
              "      <td>2.106641</td>\n",
              "      <td>0.536377</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw9U-XHcAvoT"
      },
      "source": [
        "Wait, that was actually WORSE this time?  How come? Isn't deeper better?\n",
        "\n",
        "One problem is that our [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) class includes a lot of $\\tanh$ activations internally, which have gradients tend to approach zero everywhere except near the \"middle\" -- recall the gradient of the sigmoid function we talked about in the Santa Claus example?  tanh and sigmoid are \"cousins\": In the following graph, we plot sigmoid in red, $\\tanh$ in blue, and then $(1+\\tanh)/2$ in green -- which lays right on top of the red line so you can't even see it, because they're exactly the same:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "z9OXkUkTYkP8",
        "outputId": "cac0a318-6284-4818-eae1-8b2b173a0957"
      },
      "source": [
        "HTML('<iframe src=\"https://www.desmos.com/calculator/1xlbu6tbx0?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe src=\"https://www.desmos.com/calculator/1xlbu6tbx0?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=0></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB40edypZG8N"
      },
      "source": [
        "So it's only in the region near x=0 that the gradient of tanh is significantly bigger than zero.  And with lots of tanh's feeding into other tanh's, chances are that we'll get a lot of small gradients being multiplied by other (small or large) gradients, leading to what's know as the \"vanishing gradient problem\".\n",
        "\n",
        "Actually, whenever multiple tanh functions in multiple layers are doing the same thing, we'll either get vanishing gradients or else we'll get \"exploding gradients\" when the \"middle\" parts of the tanh's line up, especially if these tanh's are \"sharpending\", i.e. become more like step functions.  \n",
        "\n",
        "What if we replaced the tanh's in the RNN with ReLU like our earlier models were using? Let's try it...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "vglQ7S0maxog",
        "outputId": "28c514b5-3aa3-4b3c-aded-d0dc6eba5e84"
      },
      "source": [
        "class LMModel5b(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True, nonlinearity='relu') # change the tanh to a relu\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h = torch.zeros(n_layers, bs, n_hidden)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        res,h = self.rnn(self.i_h(x), self.h)\n",
        "        self.h = h.detach()\n",
        "        return self.h_o(res)\n",
        "    \n",
        "    def reset(self): self.h.zero_()\n",
        "\n",
        "\n",
        "learn = Learner(dls, LMModel5b(len(vocab), 64, 2), \n",
        "                loss_func=CrossEntropyLossFlat(), \n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 3e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.222643</td>\n",
              "      <td>2.858331</td>\n",
              "      <td>0.397217</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.179497</td>\n",
              "      <td>1.889524</td>\n",
              "      <td>0.471191</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.646320</td>\n",
              "      <td>1.754807</td>\n",
              "      <td>0.518392</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.393186</td>\n",
              "      <td>1.772523</td>\n",
              "      <td>0.539307</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.125879</td>\n",
              "      <td>1.783095</td>\n",
              "      <td>0.583740</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.871718</td>\n",
              "      <td>2.140252</td>\n",
              "      <td>0.557617</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.685769</td>\n",
              "      <td>2.313240</td>\n",
              "      <td>0.673665</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.537473</td>\n",
              "      <td>2.512885</td>\n",
              "      <td>0.671305</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.437053</td>\n",
              "      <td>2.615960</td>\n",
              "      <td>0.685221</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.362013</td>\n",
              "      <td>2.838107</td>\n",
              "      <td>0.697510</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.305871</td>\n",
              "      <td>2.884769</td>\n",
              "      <td>0.680013</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.266159</td>\n",
              "      <td>2.930475</td>\n",
              "      <td>0.700684</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.239110</td>\n",
              "      <td>3.042807</td>\n",
              "      <td>0.706299</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.221393</td>\n",
              "      <td>3.053171</td>\n",
              "      <td>0.714111</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.210539</td>\n",
              "      <td>3.031907</td>\n",
              "      <td>0.717367</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1AN-O1bbMsk"
      },
      "source": [
        "Great!  That's the best so far! But still, HOW CAN WE DO BETTER?  \n",
        "One very important architecture uses the tanh instead of ReLU activation, but does so in concert with a set of tunable \"logic gates\" that allow the model to learn what's worth keeping in memory and what's worth \"forgetting\".  It's known as Long Short-Term Memory or LSTM.  These gates allow the model to manage the problems vanishing and exploding gradients. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX9O7xr4YjSq"
      },
      "source": [
        "\n",
        "## Long Short-Term Memory (LSTM)\n",
        "Here's a picture of a \"neuron\" based on LSTM.  We call it a \"cell\".  It looks a bit intimidating, but we'll unpack it below.\n",
        "\n",
        "\n",
        "![LSTM cell from fastbook](https://raw.githubusercontent.com/fastai/fastbook/780b76bef3127ce5b64f8230fce60e915a7e0735/images/LSTM.png)\n",
        "*Image source: [The fastai Book](https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb)*\n",
        "\n",
        "> Note: The following \"unpacking\" of the LSTM is (IMHO) fairly clear, but if you find it confusing, you will not be alone.  If this next part stresses you out, don't worry about it too much, and go ahead with the training code that follows.\n",
        "\n",
        "\n",
        "Importantly, the cell has not just one hidden state $h$ like before, but a new state $c$ as well, and they complement each other.  Along the bottom we see the hidden state of the cell \"h\" that persists from the previous \"time\" in the sequence (i.e. the previous token) $h_{t-1}$ to the state at the end of the cell's processing $h_t$. Note that $h_t$ is also the \"output\" of the cell, whereas $c$ is just passed along to the (later version of the) cell at a later time.  The new input at time $t$ is on the bottom as $x_t$. And rather than just adding $x$ and $h$ like we were doing before, we're going to concatenate them into one big matrix. \n",
        "\n",
        "The yellow symbols are just elementwise mathematical operations like addition, subtraction, running through tanh.  The orange symbols denote the LSTM's \"gates\", which include weights for tuning the gate -- i.e. the gates are like mini neural networks. \n",
        "\n",
        "The leftmost sigmoid is called the \"forget gate\".  The variable $c$ functions as the \"memory\" of the cell.  Based on the input $x$ and the previous hidden state $h$, the weights feeding into the forget gate determine whether to \"forget\" $c$ by multiplying it by zero, or to \"remember\" it (i.e. allow it to pass through) by multiplying it by 1, or -- and this is important -- *some number between 0 and 1* -- thus the remembering isn't an all-or-nothing proposition, but instead is *continuous* and therefore *differentiable* and therefore *amenable to training by gradient descent!* \n",
        "\n",
        "The next sigmoid is called the \"input gate\", which works in concert with the orange tanh gate (known as the \"cell gate\") which performs an operation somewhat analagous to the older RNN's tanh activation, except this is *only for modifying the cell's memory $c$.  The input gate decides how much of the new input to \"remember\". \n",
        "\n",
        "What the cell actually outputs is then in the bottom right, in which we combine some about of the old hidden state, the new input, and the new cell's memory, into a final output.  Interesting to note that the contribution from the memory $c$ can range from -1 to 1 (as determined by the yellow tanh function), whereas the part from $x$ and $h$ is only between 0 and 1 (as determined by the last sigmoid in the bottom right). \n",
        "\n",
        "The following is the code for an LSTM cell. For some readers, this may help clarify exactly what's described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tipxfxMJA7eY"
      },
      "source": [
        "class LSTMCell(Module):\n",
        "    def __init__(self, ni, nh):\n",
        "        self.ih = nn.Linear(ni,4*nh)\n",
        "        self.hh = nn.Linear(nh,4*nh)\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        h,c = state\n",
        "        # One big multiplication for all the gates is better than 4 smaller ones\n",
        "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)  # .chunk is a PyTorch method\n",
        "        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])\n",
        "        cellgate = gates[3].tanh()\n",
        "\n",
        "        c = (forgetgate*c) + (ingate*cellgate)\n",
        "        h = outgate * c.tanh()\n",
        "        return h, (h,c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeNiSkeOfpxs"
      },
      "source": [
        "The PyTorch [.chunk()](https://pytorch.org/docs/stable/generated/torch.chunk.html) just splits the tensor into (in this case) 4 chunks of length 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8slav4CBRoN"
      },
      "source": [
        "We then chain these cells together. Here's a model that does that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20vQkphdBTPW"
      },
      "source": [
        "class LMModel6(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        res,h = self.rnn(self.i_h(x), self.h)\n",
        "        self.h = [h_.detach() for h_ in h]\n",
        "        return self.h_o(res)\n",
        "    \n",
        "    def reset(self): \n",
        "        for h in self.h: h.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-e5HofHBYLC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "3025a4ca-2983-4dfc-fdf8-bd4ada35c8d8"
      },
      "source": [
        "learn = Learner(dls, LMModel6(len(vocab), 64, 2), \n",
        "                loss_func=CrossEntropyLossFlat(), \n",
        "                metrics=accuracy, cbs=ModelResetter)\n",
        "learn.fit_one_cycle(15, 1e-2)    # note that we're cranking up the learnign rate even more now!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.020345</td>\n",
              "      <td>2.683045</td>\n",
              "      <td>0.363851</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.124043</td>\n",
              "      <td>2.110640</td>\n",
              "      <td>0.297933</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.601818</td>\n",
              "      <td>1.802153</td>\n",
              "      <td>0.459473</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.317922</td>\n",
              "      <td>2.014070</td>\n",
              "      <td>0.507161</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.107276</td>\n",
              "      <td>2.052923</td>\n",
              "      <td>0.529134</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.872074</td>\n",
              "      <td>1.887227</td>\n",
              "      <td>0.612630</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.631941</td>\n",
              "      <td>1.788551</td>\n",
              "      <td>0.625651</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.436306</td>\n",
              "      <td>1.298946</td>\n",
              "      <td>0.672770</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.287841</td>\n",
              "      <td>0.922300</td>\n",
              "      <td>0.769613</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.191388</td>\n",
              "      <td>0.828305</td>\n",
              "      <td>0.790934</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.135012</td>\n",
              "      <td>0.891499</td>\n",
              "      <td>0.790120</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.099053</td>\n",
              "      <td>0.865923</td>\n",
              "      <td>0.789632</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.074308</td>\n",
              "      <td>0.924366</td>\n",
              "      <td>0.791016</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.058966</td>\n",
              "      <td>0.881077</td>\n",
              "      <td>0.799886</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.051493</td>\n",
              "      <td>0.883147</td>\n",
              "      <td>0.800374</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esssv4S0BZSi"
      },
      "source": [
        "Wonderful! that's the best so far! But WAIT, THERE'S MORE! \n",
        "\n",
        "### Regularization Methods\n",
        "\n",
        "Another thing we can do to improve the model's performance on the validation set goes under the topic of \"regularization\".  Regularization, conceptually, is any means we might try to \"make life harder\" for the training part of the code, so that it can generalize better.  Data augmentation -- such as rotating images, changing their intensity or contrast, or removing groups of pixels from the image -- is one example of regularization that we've already seen. \n",
        "\n",
        "#### Dropout\n",
        "\n",
        "Akin to removing pixels from an image in the training set (never in the validation set BTW), is to *randomly turn off neurons in the network during training*.  This is called [Dropout](https://dl.acm.org/doi/pdf/10.5555/2627435.2670313) and is a powerful method that is standard practice in Deep Learning. Dropout works by randomly turning off neurons during training, but keeping them all one when doing predictions (e.g. on the validation set).  In so doing, you make the network \"work harder\" to develop more powerful, more general, internal representations and to filter out \"noisy\" details you might not want the model to focus on. Dropout appears as a standard network layer in most DL libraries, and includes a parameter whereby you can tell it what fraction of cells in the layer to turn off during each iteration.  \n",
        "\n",
        "#### Weight Decay \n",
        "\n",
        "There's another important method called Weight Decay that gets discussed earlier in the fastai book.  Weight Decay is where you *make the magnitude of the weigths part of the loss function* -- you just add up the squares of the weights (so called \"L2 regularization\" because adding up squared things is part of an \"L2 norm\") and add that into the loss function.  The result is that the model will learn to keep the weights from getting too big, which could otherwise lead to overfitting -- again, the idea is to make things *hard* for the model while training. Schemically it looks like:\n",
        "\n",
        "```\n",
        "loss = loss + weights.pow(2).mean()*wd\n",
        "```\n",
        "where `wd` is the \"strength\" of the weight decay regularization: More means that the weights will make up a bigger part of the loss, which we're trying to *minimize* via gradient descent, so a larger `wd` parameter will lead to more minimizing of the model weights. This has the effect of reducing overfitting and (usually) improving generalization. \n",
        "\n",
        "#### Activation Regularization \n",
        "Related to weight decay is the act of making sure the neuron activations themselves don't get too large: we just add the L2 norm of the activations to the loss function as well.  To implement this we'll need to output not just the hidden state of the LSTM cell but the activations (so we can put them in the loss function).  fastai is going to handle all this for us by using the `RNNRegularizer()` callback. \n",
        "\n",
        "\n",
        "\n",
        "### Final Model: `LMModel7`\n",
        "Here's a cell with Dropout added."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2zN05hGBirh"
      },
      "source": [
        "class LMModel7(Module):\n",
        "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
        "        self.i_h = nn.Embedding(vocab_sz, n_hidden)\n",
        "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)\n",
        "        self.drop = nn.Dropout(p)\n",
        "        self.h_o = nn.Linear(n_hidden, vocab_sz)\n",
        "        self.h_o.weight = self.i_h.weight\n",
        "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        raw,h = self.rnn(self.i_h(x), self.h)\n",
        "        out = self.drop(raw)\n",
        "        self.h = [h_.detach() for h_ in h]\n",
        "        return self.h_o(out),raw,out    # the raw, out are the activations we'll use in the RNNRegularizer\n",
        "    \n",
        "    def reset(self): \n",
        "        for h in self.h: h.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9YF4dxykfys"
      },
      "source": [
        "One extra thing we did in the model above is the line that reads `self.h_o.weight = self.i_h.weight`, a regularization method called \"weight tying\".  From the fastai book:\n",
        "\n",
        "> \"Another useful trick we can add from the [AWD LSTM paper](https://arxiv.org/abs/1708.02182) is weight tying. In a language model, the input embeddings represent a mapping from English words to activations, and the output hidden layer represents a mapping from activations to English words. We might expect, intuitively, that these mappings could be the same.\"\n",
        "\n",
        "With all this then, here's (almost) our last from-scratch-weighted learner of the lesson:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5os9xo7Bo12"
      },
      "source": [
        "learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n",
        "                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n",
        "                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axcD7i4XlD4m"
      },
      "source": [
        "Which, incidentally there's a special kind of Learner class defined that incldues some of this automatically:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2ZrHUVglJ-H"
      },
      "source": [
        "learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n",
        "                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1pafVSwlO0E"
      },
      "source": [
        "Then we can do the training and pass in a weight decay strength of 0.1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "b244BsUAlOQn",
        "outputId": "f40804c3-6e6b-451a-cd33-a88a9ced983a"
      },
      "source": [
        "learn.fit_one_cycle(15, 1e-2, wd=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.477206</td>\n",
              "      <td>1.809246</td>\n",
              "      <td>0.476644</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.614283</td>\n",
              "      <td>1.281632</td>\n",
              "      <td>0.612793</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.878191</td>\n",
              "      <td>0.883364</td>\n",
              "      <td>0.778971</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.431828</td>\n",
              "      <td>0.635024</td>\n",
              "      <td>0.840007</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.211996</td>\n",
              "      <td>0.594336</td>\n",
              "      <td>0.860677</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.114491</td>\n",
              "      <td>0.688404</td>\n",
              "      <td>0.827555</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.065761</td>\n",
              "      <td>0.531600</td>\n",
              "      <td>0.872803</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.043977</td>\n",
              "      <td>0.462177</td>\n",
              "      <td>0.876546</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.032514</td>\n",
              "      <td>0.451317</td>\n",
              "      <td>0.886393</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.025291</td>\n",
              "      <td>0.450308</td>\n",
              "      <td>0.888591</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.020762</td>\n",
              "      <td>0.490371</td>\n",
              "      <td>0.880941</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.017227</td>\n",
              "      <td>0.456757</td>\n",
              "      <td>0.880615</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.014316</td>\n",
              "      <td>0.486691</td>\n",
              "      <td>0.877360</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.012070</td>\n",
              "      <td>0.469082</td>\n",
              "      <td>0.879964</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.010876</td>\n",
              "      <td>0.461661</td>\n",
              "      <td>0.881836</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0kNL1aaBnXP"
      },
      "source": [
        "88% in 15 seconds, training from scratch?!  Wow, pretty good, eh?! \n",
        "\n",
        "And yet, as we said before, learning the English language involves a lot more than just being able to parse numbers in natural language form. So for greater sophistication, we're going to switch datasets, switch models, and use Transfer Learning instead of training from scratch. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4bJoZSimjr4"
      },
      "source": [
        "---\n",
        "\n",
        "# Part IV - Using Language Model Weights for Other Things\n",
        "\n",
        "Earlier we looked at the IMDB dataset for movie reviews. Let's re-run the same code we did in Part I before to set up what comes next:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQCPD_1ecDSO"
      },
      "source": [
        "# in case you're coming back to Colab and want to restart without scrolling up,\n",
        "# uncomment the following lines:\n",
        "!pip install -Uqq fastai fastbook"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNxho6R4cUJ3"
      },
      "source": [
        "import fastbook  \n",
        "from fastai.text.all import *\n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGAif4oIvnBs"
      },
      "source": [
        "All the tokenization and numericalization and special \"continue-along-rows-between-batches\" slicing that  we've done earlier in Part I happens under the hood in the fastai [TextBlock](https://docs.fast.ai/text.data.html#TextBlock)*italicized text* special class which can be fed into the more generic Datablock class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EuaxMqio2gx"
      },
      "source": [
        "\n",
        "### How much time do you have to get good results?\n",
        "\n",
        "There are two different-sized IMDB datasets.  The original fastai lesson -- which gives great results -- trains on the full 140 MB IMDB movie review dataset.  There's much smaller sample dataset which is NOT supposed to be great for language model training but it'll go a heck of a lot faster.  In what follows, I'm including both options.  \n",
        "\n",
        "\n",
        "I recommend that you leave `full_dataset` set to `False` at first --- so that you can run the following code in **a few minutes** rather than **6 HOURS** --- and then we'll load the fully-trained model weights below as with the \"cooking show\" metaphor mentioned earlier.  Feel free to come back later and change `full_dataset` to `True` and run it yourself if you want (but you need to). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "A2fShulUeo_4",
        "outputId": "45240838-53c1-4cf5-ac7b-003971c588f2"
      },
      "source": [
        "\n",
        "def setup_imdb_dataloaders(full_dataset=False, bs=128):\n",
        "    \"made this a function instead of a cell so we can call it again below\"\n",
        "\n",
        "    if full_dataset:  # Full IMDB dataset as per fastbook Chapter 10; everything will take long\n",
        "\n",
        "        path = untar_data(URLs.IMDB)  # Full IMDB dataset, ~140 MB in size \n",
        "\n",
        "        # get_imdb is a data-getter function, where path is passed in below\n",
        "        get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup']) \n",
        "\n",
        "        dls_lm = DataBlock(  \n",
        "            blocks=TextBlock.from_folder(path, is_lm=True),\n",
        "            get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
        "        ).dataloaders(path, path=path, bs=bs, seq_len=80)\n",
        "\n",
        "    else:  # Faster, won't produce as good end results. \n",
        "\n",
        "        path = untar_data(URLs.IMDB_SAMPLE)  # smaller IMDB examples, 558K\n",
        "\n",
        "        # IMDB_SAMPLE is a .csv, so follow https://docs.fast.ai/text.data.html#TextBlock.from_df \n",
        "        df = pd.read_csv(path/'texts.csv')\n",
        "\n",
        "        db_lm = DataBlock(\n",
        "            blocks=TextBlock.from_df('text', is_lm=True),\n",
        "            get_x=ColReader('text'), splitter=RandomSplitter(0.1))\n",
        "        dls_lm = db_lm.dataloaders(df, bs=bs, seq_len=80)\n",
        "\n",
        "    return path, dls_lm \n",
        "\n",
        "\n",
        "full_dataset = False   # change to True if you want to train this thing yourself (~5 hours on Colab)\n",
        "bs = 64   # batch size. if you get out of memory errors in training, cut this in half, restart & retry\n",
        "\n",
        "\n",
        "path, dls_lm = setup_imdb_dataloaders(full_dataset=full_dataset, bs=bs)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da7EFXN4fxQw"
      },
      "source": [
        "Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "kX0DrM-Bfwyn",
        "outputId": "268fc1c1-e090-47a7-f684-b68945bc3d6a"
      },
      "source": [
        "dls_lm.show_batch(max_n=2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj want to watch a scary horror film ? xxmaj then steer clear of this one . xxmaj there 's not enough beer in the world to make this film enjoyable . \\n\\n xxmaj however , there is enough xxunk . xxmaj single - xxunk , if you can manage it . \\n\\n xxmaj if the previous comments were n't enough to keep you from watching this film xxunk , allow me to xxunk . xxup nasa xxunk one</td>\n",
              "      <td>xxmaj want to watch a scary horror film ? xxmaj then steer clear of this one . xxmaj there 's not enough beer in the world to make this film enjoyable . \\n\\n xxmaj however , there is enough xxunk . xxmaj single - xxunk , if you can manage it . \\n\\n xxmaj if the previous comments were n't enough to keep you from watching this film xxunk , allow me to xxunk . xxup nasa xxunk one man</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>design , having no real impact on the story . xxmaj i 'd argue that the whole point of using drawn animation ( instead of actors / xxup cgi ) is to really push the limits of imagination and design ; to do that which is too difficult / xxunk in other xxunk . xxmaj although the animation in xxmaj renaissance is certainly stunning and incredibly well - accomplished , i never felt like i was seeing something that has</td>\n",
              "      <td>, having no real impact on the story . xxmaj i 'd argue that the whole point of using drawn animation ( instead of actors / xxup cgi ) is to really push the limits of imagination and design ; to do that which is too difficult / xxunk in other xxunk . xxmaj although the animation in xxmaj renaissance is certainly stunning and incredibly well - accomplished , i never felt like i was seeing something that has n't</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLKmQDzkf_VC"
      },
      "source": [
        "### Fine-Tuning the Pretrained Language Model\n",
        "\n",
        "The pretrained model we're going to use is an LSTM called \"AWS_LSTM\" that was pretrained by Howard et al on Wikipedia. Earlier in the course when we were working with images, we used a fastai Learner called `cnn_learner` that loaded a pretrained \"ResNet\" model, so this is similar: There's `language_model_learner()` we can call:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoH4LBPQgjv4"
      },
      "source": [
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
        "    metrics=[accuracy, Perplexity()]).to_fp16()   # fp16, if our GPU will support it natively, will help this run faster"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdBXUUCFgqgm"
      },
      "source": [
        "That progress bar you just saw was a download the pretrained model weights.  When we train the model in the next cell, the progress bar you'll see is going to be for *just one epoch* through this dataset, because the IMDB dataset is huge.  \n",
        "\n",
        "> **Warning: If you use the full IMDB dataset** (`full_dataset=True`) **then expect the next cell to take 20 to 30 minutes to run!** \n",
        "\n",
        "(If you're not using Colab and you get CUDA Out of Memory error, then go back and decrease the batch size in the dataloders definition, e.g. `bs=32`.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Gwp65QCjg6ET",
        "outputId": "4c5ac10f-8a2d-4c40-90cf-fa2529ac8493"
      },
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.340217</td>\n",
              "      <td>4.236506</td>\n",
              "      <td>0.268823</td>\n",
              "      <td>69.165794</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dMh4qlPtnYS"
      },
      "source": [
        "What's that \"perplexity\" metric?  From the fastai book:\n",
        "> \"The perplexity metric used here is often used in NLP for language models: it is the exponential of the loss (i.e., `torch.exp(cross_entropy)`). We also include the accuracy metric, to see how many times our model is right when trying to predict the next word, since cross-entropy (as we've seen) is both hard to interpret, and tells us more about the model's confidence than its accuracy.\" \n",
        "\n",
        "Since perplexity scales with the loss, that means that ***lower values are better***. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbtoYuEahh-X"
      },
      "source": [
        "At this point, the fastai book (Chapter 10) notes that it's high time we learn about saving model checkpoints, so that we can resume our work later if something happens, and reload a model we were training.  With fastai it's as simple as `learn.save(<filename>)` where the actual file gets a `.pth` appended to it, and it goes in a new directory called `models/` off of wherever the learner's `path` is currently set to:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtujM-xDh-uU",
        "outputId": "16090a03-2174-417b-a82f-b23080f4a586"
      },
      "source": [
        "learn.save('1epoch')  # this will create <learn.path/>models/1epoch.pth"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('models/1epoch.pth')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRnGpwu7iOdR"
      },
      "source": [
        "Loading the trained model back is as simple as `learn.load()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkFDD7xbiTd7"
      },
      "source": [
        "learn = learn.load('1epoch')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB8lVM8giVpL"
      },
      "source": [
        "> Note: When you're using `learn.load()`, you need to have *already defined* `learn` *and its model*: all the `load()` function does is overwrite the model weights and the state of the optimizer, and a few other things. (Think of it this way, in order to call \"learn.load()\", `learn` needs to be a defined variable already!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFWGeUNQjHBX"
      },
      "source": [
        "Now, by default when you load a model in fastai, the weights are \"frozen\" except for the very last layer (and maybe a few of the other later layers, depending on the specific sub-class of Learner you're using).  Frozen means that most of the weights in the network are not training at all, only the last (few) layer(s).  This is to fine-tune the model, as it's been found that the most generic representations in the network happen in the early layers and are probably a great guess to get started, whereas the later layers are closer to new data and will need to evolve.\n",
        "\n",
        "We could train all the model's layers at once in a \"unfrozen\" state --- and we're about to do just that -- but experience with Transfer Learning has shown that by doing this (mostly) frozen pre-training step is better because it doesn't cause the earlier layers to fluctuate wildly in response to the later layers being initialized from scratch. \n",
        "\n",
        "That said, unfreezing the whole model means it'll take longer to train because we'll now be computing *gradients* for everything instead of just the later layers.  In many cases though, it doesn't take much longer, e.g. usually only about 20% longer (or less) than the frozen training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "sRRwtbk3kDhG",
        "outputId": "ca757462-7e70-482e-fbd7-604e6906f104"
      },
      "source": [
        "learn.unfreeze()\n",
        "\n",
        "learn.fit_one_cycle(10, 2e-3)\n",
        "\n",
        "learn.save('finetuned_full')\n",
        "\n",
        "# output immediately below is shown for full_dataset=False"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.137601</td>\n",
              "      <td>4.090182</td>\n",
              "      <td>0.283821</td>\n",
              "      <td>59.750782</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.041873</td>\n",
              "      <td>4.033553</td>\n",
              "      <td>0.287096</td>\n",
              "      <td>56.461143</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.921970</td>\n",
              "      <td>4.027807</td>\n",
              "      <td>0.285063</td>\n",
              "      <td>56.137680</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.750067</td>\n",
              "      <td>4.066389</td>\n",
              "      <td>0.281642</td>\n",
              "      <td>58.345871</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.544308</td>\n",
              "      <td>4.154185</td>\n",
              "      <td>0.274848</td>\n",
              "      <td>63.700016</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.313109</td>\n",
              "      <td>4.279891</td>\n",
              "      <td>0.267754</td>\n",
              "      <td>72.232536</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.081216</td>\n",
              "      <td>4.418332</td>\n",
              "      <td>0.259444</td>\n",
              "      <td>82.957764</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.873045</td>\n",
              "      <td>4.520293</td>\n",
              "      <td>0.256298</td>\n",
              "      <td>91.862488</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.697660</td>\n",
              "      <td>4.572013</td>\n",
              "      <td>0.253701</td>\n",
              "      <td>96.738640</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.581851</td>\n",
              "      <td>4.581793</td>\n",
              "      <td>0.253474</td>\n",
              "      <td>97.689377</td>\n",
              "      <td>00:11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('models/finetuned_full.pth')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v75Wj5XmvDEX"
      },
      "source": [
        "Expected output for the above when `full_dataset=True`: \n",
        "\n",
        "(copied from my run on a GTX 3080 GPU, where I needed `bs=64` instead of 128 for memory reasons):\n",
        "\n",
        "```\n",
        "epoch train_loss valid_loss  accuracy    perplexity   time\n",
        "--   --------    --------    --------    ---------    -----\n",
        "0\t3.778447\t3.754353\t0.317398\t42.706593\t11:45\n",
        "1\t3.741970\t3.716871\t0.321567\t41.135479\t11:36\n",
        "2\t3.654867\t3.671912\t0.326534\t39.327045\t11:35\n",
        "3\t3.607213\t3.639998\t0.330397\t38.091766\t11:38\n",
        "4\t3.534192\t3.614742\t0.334226\t37.141747\t11:34\n",
        "5\t3.478336\t3.590213\t0.337204\t36.241806\t11:38\n",
        "6\t3.395994\t3.572024\t0.339824\t35.588547\t11:33\n",
        "7\t3.321614\t3.564811\t0.341225\t35.332790\t11:37\n",
        "8\t3.243030\t3.566489\t0.341502\t35.392105\t11:28\n",
        "9\t3.223321\t3.571197\t0.341280\t35.559132\t11:37\n",
        "```\n",
        "(and note that Colab is 3x slower than the GPU used here, so that would amount to 5 to 6 hours of training on Colab.) \n",
        "\n",
        "So, from the \"expected output\" for the full IMDB dataset (`full_dataset=True`) and training for 5 hours, one sees an accuracy of around 0.34 and a perplexity of about 35.  The fact that the perplexity for the small (`full_dataset=False`) method comes out to around 103 indicates that the smaller dataset is NOT going to be good for modeling language.  Also note that the `valid_loss` flattens out whereas the `train_loss` keeps decreasing, indicating that we are overfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r2Q1vJdujJE"
      },
      "source": [
        "### Cooking Show Approach: ...and we're back from commercial!\n",
        "Taking the \"cooking show\" approach, let's say you did all that training on the larger dataset and now we're ready to \"pull it out of the oven\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEKaeh7algZR",
        "outputId": "4c15ca3e-d8d0-43c4-ba89-f92decc655fb"
      },
      "source": [
        "## COOKING SHOW: ...AND we're back from commercial and everything's finished! \n",
        "\n",
        "# If you skipped the above training (which is fine!), then download & load the \n",
        "# (~500 MB) weights from when I ran it:\n",
        "% pip install -Uqq mrspuff\n",
        "from mrspuff.scrape import download \n",
        "\n",
        "my_pretrained_url = 'https://www.dropbox.com/s/ozyhw44argi5th1/finetuned_full.pth?dl=1'\n",
        "local_file = str(learn.path) + '/models/finetuned_full.pth'\n",
        "if not os.path.exists(local_file): download(my_pretrained_url, local_file) \n",
        "\n",
        "path, dls_lm = setup_imdb_dataloaders(full_dataset=True, bs=bs)  # be sure we have the full IMDB dataset\n",
        "\n",
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3, \n",
        "    metrics=[accuracy, Perplexity()]).to_fp16()   # fp16, if our GPU will support it natively, will help this run faster\n",
        "\n",
        "learn.load('finetuned_full')\n",
        "print(\"Success! Fine-tuned weights loaded.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Fine-tuned weights loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xzpdF0llJwT"
      },
      "source": [
        "In addition to the usual `learn.save()`, the `language_model_learner` comes with its own special `.save_encoder()` method that will just save the \"body\" of the model without the predictive text head.  We will use this body for the text classification task, after the following interlude:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAadORVFlZ7L"
      },
      "source": [
        "learn.save_encoder('finetuned_encoder')  # we'll use this after the interlude"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncftPiRfXEuq"
      },
      "source": [
        "## Interlude!  Let's Generate Text!\n",
        "Before we get to text classification, let's have some fun and make our language model generate text!  To do this we feed the model's own predictions back in as inputs and have continue on predicting word after word.  We could write this part of the code from scratch like before, but in this cast fastai's `language_model_learner` automatically knows to do this if we ask it to keep predicting a long time. \n",
        "\n",
        "Part of the prediction is a parameter called \"temperature\" which is related to how temperature affects the probability distrubution of molecule speeds in physics and chemistry: the higher the temperature, the greater the varibility in the model's outputs. This feature is part of many generative language models so that rather than deterministically outputting the same thing each time, the output is *sampled* from a probability distribution that is skewed by the temperature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "991FQWYrZwY4",
        "outputId": "acdbf923-31bd-4117-e3d1-2a776d47d2d5"
      },
      "source": [
        "def generate_text(prompt, n_words=40, n_sentences=2, temperature=0.75):\n",
        "\n",
        "    preds = [learn.predict(prompt, n_words, temperature=temperature) \n",
        "            for _ in range(n_sentences)]\n",
        "    return \"\\n\".join(preds)  \n",
        "\n",
        "prompt = \"I liked this movie because\"\n",
        "print(generate_text(prompt))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i liked this movie because i expected it to be a mystery and i hope that this one for me will be a good one for this movie . Not as room as i thought it was . \n",
            "\n",
            " i girl not sure how\n",
            "i liked this movie because i watched it in Paris and Paris in a small English town . The scenes in the movie were great in the early beings ( when Paris Crippled and Deeds Scarlet were\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z29VBaOSbRHO"
      },
      "source": [
        "Note how we got multiple different outputs due to the random sampling.\n",
        "\n",
        "\n",
        "We can make it generate longer text too:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "4kUS6IsmbWLj",
        "outputId": "5dfa6b30-7985-4e3a-d3ed-a8f1dd1a4686"
      },
      "source": [
        "prompt = \"It's strange that people are going nuts over Squid Game because\"\n",
        "print( generate_text(prompt, n_words=75) )"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It 's strange that people are going nuts over Squid Game because they all dinner to love one another . The music of this song is just as much a part of the movie as a movie . \n",
            "\n",
            " Nobody knows where Squid / Squid powerful . \n",
            "\n",
            " The first i can say Squid was a genius , a genius , and now he is only the one who has a whole private to it , but it 's simply a\n",
            "It 's strange that people are going nuts over Squid Game because it 's just for a movie . It 's a perfectly Hollywood movie , but it hurts to say that this movie is really short to the Hollywood subtitles . There are many minor absolute in this movie . Nothing to do with this movie , a good thriller is n't enough , but it 's certainly worth watching when you 're not worth awaiting your money because it is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg0Ll5D8cF0u"
      },
      "source": [
        "Go ahead and try your own prompt below.  Also try changing the default value of the temperature (to other values between 0 and 1) to observe its effect on the generated text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "-oh3K6HHcId9",
        "outputId": "5c299a4f-c687-4e6b-e455-ef296cacf86c"
      },
      "source": [
        "prompt = \"YOUR PROMPT HERE\"\n",
        "print( generate_text(prompt, temperature=0.9) )"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOUR PROMPT HERE is a truly excellent story . \n",
            "\n",
            " This is a season show The Russia Glue . This story is told from the point of view of the main character . It 's a perfect\n",
            "YOUR PROMPT HERE : The Duck and Duck ( i will dinner it , contains a experiences of duck or people are called for they were one of the first dark COHEN applause shown in my life ) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsiQSNF5bMh7"
      },
      "source": [
        "Try your own prompt below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjXdpSdKXX6S"
      },
      "source": [
        "## Now For the Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtPzuTjhqetA"
      },
      "source": [
        "## Ungraded Exercise: Compare Against Non-Pretrained Model\n",
        "\n",
        "Repeat the above training after defining the learner *again* (i.e. just copy & paste the learner definition from above) only  this time *don't* load from the checkpoint of the language model, just train it \"from scratch\".  Compare the performance to what we got above.  Note \n",
        "- how the earlier model already starts with a much higher accuracy.\n",
        "- how this \"from scratch\" can overfit before it starts reaching the accuracy of the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpBVlQynrGsH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT2Mm2Vx7jvA"
      },
      "source": [
        "---\n",
        "Acknowledgements: In addition to the liberal re-use of Howard & Gugger's fastai book code, the author wishes to acknowledge Zach Mueller for helpful coaching via the fastai Discord on the various NLP datasets, their sizes, and what they're good for. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btk6jA7Q77Xg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}